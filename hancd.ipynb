{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd8d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Concatenate, BatchNormalization, Activation,LayerNormalization\n",
    "from keras.layers import Lambda, Embedding, GRU, Bidirectional, TimeDistributed, concatenate\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from word2vecReader import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "# from tensorflow.python.keras.optimizers import adam_v2\n",
    "# from tensorflow.python.keras.optimizers import rmsprop_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from  keras import layers\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffadec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe32da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(1)\n",
    "# K.set_learning_phase(True)   # <==\n",
    "np.random.seed(0)\n",
    "MAX_SENT_LENGTH = 20###number of words in a sentence\n",
    "MAX_NB_WORDS = 20000\n",
    "POST_DIM = 400\n",
    "INFO_DIM=30\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86100bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "\n",
    "    return Lambda(func)\n",
    "\n",
    "\n",
    "def myFunc(x):\n",
    "    if \"empety\" in x:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = string.strip().lower()\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_words = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def find_str(s, char):\n",
    "    index = 0\n",
    "\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index + len(char)] == char:\n",
    "                    return index\n",
    "\n",
    "            index += 1\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weightss = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4debe465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.610000e+02 4.265330e+05 3.790000e+02 2.102000e+03]\n",
      " [6.980000e+02 1.974089e+06 3.890000e+02 1.089390e+05]\n",
      " [1.257000e+03 3.495940e+05 6.560000e+02 6.441000e+03]\n",
      " ...\n",
      " [1.905000e+03 1.180000e+03 2.610000e+02 5.000000e+01]\n",
      " [3.190000e+02 1.426500e+04 4.850000e+02 4.540000e+02]\n",
      " [2.009000e+03 8.882920e+05 3.180000e+02 3.773400e+04]]\n",
      "Total 8009 unique tokens.\n",
      "Shape of data tensor: (2121, 192, 21)\n",
      "Shape of label tensor: (2121, 2)\n"
     ]
    }
   ],
   "source": [
    "with open('instagram.pickle', 'rb') as handle:\n",
    "    dictionary = pickle.load(handle)\n",
    "texts = dictionary['text']\n",
    "texts = texts.fillna(\"\")\n",
    "# texts=[text.encode('ascii') for text in texts]\n",
    "#print(texts)\n",
    "comments = dictionary['comments']\n",
    "#print(comments)\n",
    "timeInfo = dictionary['time']\n",
    "#print(timeInfo)\n",
    "postInfo = dictionary['post']\n",
    "#print(postInfo)\n",
    "labels = dictionary['labels']\n",
    "#print(labels)\n",
    "b = np.zeros([len(timeInfo), len(max(timeInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(timeInfo):\n",
    "    b[i][0:len(j)] = j\n",
    "timeInfo = b\n",
    "#print(b)\n",
    "time_size = len(np.unique(timeInfo))\n",
    "MAX_SENTS = len(timeInfo[0])  ####number of sentences\n",
    "\n",
    "c = np.zeros([len(postInfo), len(max(postInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(postInfo):\n",
    "    c[i][0:len(j)] = j\n",
    "median_value = np.median(c)\n",
    "c = np.where(c > 10000000,median_value , c)\n",
    "postInfo = c\n",
    "print(postInfo)\n",
    "#print(postInfo)\n",
    "post_size = len(np.unique(postInfo))\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "#print(MAX_SENTS)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH+1), dtype='int32')\n",
    "#print(tokenizer.word_index['club'])\n",
    "\n",
    "for i, sentences in enumerate(comments):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for word in wordTokens:\n",
    "                #print(type(wordTokens[0]))\n",
    "                #print(\"'{}'\".format(word))\n",
    "                if k < MAX_SENT_LENGTH and word in tokenizer.word_index:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "#print(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "single_label = np.asarray(labels)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "###standardize\n",
    "transfer = StandardScaler()\n",
    "timeInfo = transfer.fit_transform(timeInfo)\n",
    "postInfo = transfer.fit_transform(postInfo)\n",
    "HAN_pre = []\n",
    "HAN_reca = []\n",
    "HAN_f1 = []\n",
    "HAN_AUC = []\n",
    "HAN_TIME = []\n",
    "embeddings_index = Word2Vec.load_word2vec_format(\"word2vec_twitter_model.bin\", binary=True, )  #\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, POST_DIM))\n",
    "outword_dic = dict()\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        new_vector = np.random.rand(POST_DIM, )\n",
    "        outword_dic.setdefault(word, new_vector)\n",
    "        embedding_matrix[i] = outword_dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d20ce62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
      " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0\n",
      " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
      " 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1166.  531.]\n",
      "[296. 128.]\n",
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_4/Sigmoid:0', description=\"created by layer 'activation_4'\")\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 192, 22)     88          ['input_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_4[0][0]']  \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 30)           120         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 192, 400)    724800      ['time_distributed_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 30)          120         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " att_layer_3 (AttLayer)         (None, 400)          80400       ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 30)           0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 192, 1)       400         ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 430)          0           ['att_layer_3[0][0]',            \n",
      "                                                                  'activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 192, 1)      4           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2)            860         ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 192, 1)       0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 2)           8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 193, 1)       0           ['input_7[0][0]',                \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 2)            0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 192, 1)       0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 224s 8s/step - loss: 0.7597 - activation_5_loss: 0.7597 - lambda_5_loss: 1.1093\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 210s 8s/step - loss: 0.7300 - activation_5_loss: 0.7300 - lambda_5_loss: 1.1203\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.7219 - activation_5_loss: 0.7218 - lambda_5_loss: 1.1284\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 216s 8s/step - loss: 0.7210 - activation_5_loss: 0.7210 - lambda_5_loss: 1.1296\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 216s 8s/step - loss: 0.7162 - activation_5_loss: 0.7162 - lambda_5_loss: 1.1413\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 216s 8s/step - loss: 0.7177 - activation_5_loss: 0.7177 - lambda_5_loss: 1.1428\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7187 - activation_5_loss: 0.7187 - lambda_5_loss: 1.1469\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7084 - activation_5_loss: 0.7084 - lambda_5_loss: 1.1340\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7204 - activation_5_loss: 0.7203 - lambda_5_loss: 1.1442\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7116 - activation_5_loss: 0.7115 - lambda_5_loss: 1.1406\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7090 - activation_5_loss: 0.7089 - lambda_5_loss: 1.1475\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7101 - activation_5_loss: 0.7101 - lambda_5_loss: 1.1468\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7113 - activation_5_loss: 0.7112 - lambda_5_loss: 1.1503\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7080 - activation_5_loss: 0.7080 - lambda_5_loss: 1.1515\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7075 - activation_5_loss: 0.7074 - lambda_5_loss: 1.1425\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7080 - activation_5_loss: 0.7079 - lambda_5_loss: 1.1576\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7100 - activation_5_loss: 0.7100 - lambda_5_loss: 1.1526\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7077 - activation_5_loss: 0.7077 - lambda_5_loss: 1.1524\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7066 - activation_5_loss: 0.7066 - lambda_5_loss: 1.1492\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7059 - activation_5_loss: 0.7059 - lambda_5_loss: 1.1534\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.7074 - activation_5_loss: 0.7074 - lambda_5_loss: 1.1505\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7100 - activation_5_loss: 0.7100 - lambda_5_loss: 1.1563\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7039 - activation_5_loss: 0.7039 - lambda_5_loss: 1.1653\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 217s 8s/step - loss: 0.6999 - activation_5_loss: 0.6999 - lambda_5_loss: 1.1442\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.7035 - activation_5_loss: 0.7034 - lambda_5_loss: 1.1591\n",
      "14/14 [==============================] - 24s 1s/step\n",
      "cpu_time:\n",
      "70823.203125\n",
      "f1:\n",
      "(array([0.75399361, 0.45945946]), array([0.7972973, 0.3984375]), array([0.77504105, 0.42677824]), array([296, 128], dtype=int64))\n",
      "auc:\n",
      "0.5978673986486486\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1155.  542.]\n",
      "[307. 117.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_7/Sigmoid:0', description=\"created by layer 'activation_7'\")\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 192, 22)     88          ['input_10[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_8[0][0]']  \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 30)           120         ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 192, 400)    724800      ['time_distributed_2[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 30)          120         ['dense_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_5 (AttLayer)         (None, 400)          80400       ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 30)           0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 192, 1)       400         ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 430)          0           ['att_layer_5[0][0]',            \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 192, 1)      4           ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 2)            860         ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 192, 1)       0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 2)           8           ['dense_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 193, 1)       0           ['input_11[0][0]',               \n",
      "                                                                  'activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 2)            0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 192, 1)       0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 223s 8s/step - loss: 0.7529 - activation_8_loss: 0.7529 - lambda_8_loss: 1.3896\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 212s 8s/step - loss: 0.7054 - activation_8_loss: 0.7053 - lambda_8_loss: 1.3665\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6821 - activation_8_loss: 0.6821 - lambda_8_loss: 1.3524\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6734 - activation_8_loss: 0.6734 - lambda_8_loss: 1.3382\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6724 - activation_8_loss: 0.6724 - lambda_8_loss: 1.3255\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6634 - activation_8_loss: 0.6633 - lambda_8_loss: 1.3185\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6643 - activation_8_loss: 0.6642 - lambda_8_loss: 1.3192\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6611 - activation_8_loss: 0.6611 - lambda_8_loss: 1.3082\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6562 - activation_8_loss: 0.6562 - lambda_8_loss: 1.2939\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6591 - activation_8_loss: 0.6591 - lambda_8_loss: 1.2956\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6589 - activation_8_loss: 0.6588 - lambda_8_loss: 1.3030\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6524 - activation_8_loss: 0.6524 - lambda_8_loss: 1.2946\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6573 - activation_8_loss: 0.6572 - lambda_8_loss: 1.2915\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6503 - activation_8_loss: 0.6502 - lambda_8_loss: 1.2911\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6560 - activation_8_loss: 0.6559 - lambda_8_loss: 1.3038\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6555 - activation_8_loss: 0.6555 - lambda_8_loss: 1.2885\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6511 - activation_8_loss: 0.6511 - lambda_8_loss: 1.2966\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6496 - activation_8_loss: 0.6496 - lambda_8_loss: 1.2880\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6511 - activation_8_loss: 0.6510 - lambda_8_loss: 1.2907\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6535 - activation_8_loss: 0.6535 - lambda_8_loss: 1.2813\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6506 - activation_8_loss: 0.6506 - lambda_8_loss: 1.2742\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 213s 8s/step - loss: 0.6484 - activation_8_loss: 0.6484 - lambda_8_loss: 1.2784\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6508 - activation_8_loss: 0.6507 - lambda_8_loss: 1.2679\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.6545 - activation_8_loss: 0.6544 - lambda_8_loss: 1.2969\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 213s 8s/step - loss: 0.6466 - activation_8_loss: 0.6466 - lambda_8_loss: 1.2711\n",
      "14/14 [==============================] - 33s 2s/step\n",
      "cpu_time:\n",
      "71433.125\n",
      "f1:\n",
      "(array([0.79347826, 0.40540541]), array([0.71335505, 0.51282051]), array([0.75128645, 0.45283019]), array([307, 117], dtype=int64))\n",
      "auc:\n",
      "0.6130877808402238\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
      " 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      " 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1175.  522.]\n",
      "[287. 137.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_10/Sigmoid:0', description=\"created by layer 'activation_10'\")\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 192, 22)     88          ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_12[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 30)           120         ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirectional  (None, 192, 400)    724800      ['time_distributed_3[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 30)          120         ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_7 (AttLayer)         (None, 400)          80400       ['bidirectional_7[0][0]']        \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 30)           0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 192, 1)       400         ['bidirectional_7[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 430)          0           ['att_layer_7[0][0]',            \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 192, 1)      4           ['dense_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 2)            860         ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 192, 1)       0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 2)           8           ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 193, 1)       0           ['input_15[0][0]',               \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 2)            0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_11 (Lambda)             (None, 192, 1)       0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 243s 9s/step - loss: 0.7254 - activation_11_loss: 0.7253 - lambda_11_loss: 1.9096\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 233s 9s/step - loss: 0.6911 - activation_11_loss: 0.6910 - lambda_11_loss: 1.7180\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6844 - activation_11_loss: 0.6843 - lambda_11_loss: 1.6273\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6760 - activation_11_loss: 0.6759 - lambda_11_loss: 1.5604\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 233s 9s/step - loss: 0.6773 - activation_11_loss: 0.6773 - lambda_11_loss: 1.5093\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 233s 9s/step - loss: 0.6688 - activation_11_loss: 0.6688 - lambda_11_loss: 1.4478\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6644 - activation_11_loss: 0.6644 - lambda_11_loss: 1.4288\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6688 - activation_11_loss: 0.6688 - lambda_11_loss: 1.3992\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6701 - activation_11_loss: 0.6701 - lambda_11_loss: 1.3760\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 235s 9s/step - loss: 0.6609 - activation_11_loss: 0.6609 - lambda_11_loss: 1.3597\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6611 - activation_11_loss: 0.6611 - lambda_11_loss: 1.3404\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6638 - activation_11_loss: 0.6637 - lambda_11_loss: 1.3306\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6611 - activation_11_loss: 0.6610 - lambda_11_loss: 1.3282\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6571 - activation_11_loss: 0.6571 - lambda_11_loss: 1.3136\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6565 - activation_11_loss: 0.6565 - lambda_11_loss: 1.3000\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6557 - activation_11_loss: 0.6557 - lambda_11_loss: 1.3002\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6606 - activation_11_loss: 0.6606 - lambda_11_loss: 1.2915\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 235s 9s/step - loss: 0.6588 - activation_11_loss: 0.6587 - lambda_11_loss: 1.2871\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6547 - activation_11_loss: 0.6547 - lambda_11_loss: 1.2796\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6550 - activation_11_loss: 0.6550 - lambda_11_loss: 1.2811\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6579 - activation_11_loss: 0.6579 - lambda_11_loss: 1.2728\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6584 - activation_11_loss: 0.6583 - lambda_11_loss: 1.2682\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6517 - activation_11_loss: 0.6517 - lambda_11_loss: 1.2623\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6530 - activation_11_loss: 0.6530 - lambda_11_loss: 1.2616\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 234s 9s/step - loss: 0.6518 - activation_11_loss: 0.6518 - lambda_11_loss: 1.2623\n",
      "14/14 [==============================] - 26s 2s/step\n",
      "cpu_time:\n",
      "77328.421875\n",
      "f1:\n",
      "(array([0.73602484, 0.50980392]), array([0.82578397, 0.37956204]), array([0.77832512, 0.43514644]), array([287, 137], dtype=int64))\n",
      "auc:\n",
      "0.6026730079605279\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1\n",
      " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1178.  519.]\n",
      "[284. 140.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_13/Sigmoid:0', description=\"created by layer 'activation_13'\")\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 192, 22)     88          ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_16[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 30)           120         ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_9 (Bidirectional  (None, 192, 400)    724800      ['time_distributed_4[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 30)          120         ['dense_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_9 (AttLayer)         (None, 400)          80400       ['bidirectional_9[0][0]']        \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 30)           0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 192, 1)       400         ['bidirectional_9[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 430)          0           ['att_layer_9[0][0]',            \n",
      "                                                                  'activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 192, 1)      4           ['dense_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 2)            860         ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 192, 1)       0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 2)           8           ['dense_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 193, 1)       0           ['input_19[0][0]',               \n",
      "                                                                  'activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 2)            0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (None, 192, 1)       0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 268s 9s/step - loss: 0.7088 - activation_14_loss: 0.7088 - lambda_14_loss: 1.0689\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6677 - activation_14_loss: 0.6677 - lambda_14_loss: 0.9975\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6587 - activation_14_loss: 0.6587 - lambda_14_loss: 0.9293\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6539 - activation_14_loss: 0.6538 - lambda_14_loss: 0.8900\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6505 - activation_14_loss: 0.6504 - lambda_14_loss: 0.8934\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6446 - activation_14_loss: 0.6446 - lambda_14_loss: 0.9211\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6445 - activation_14_loss: 0.6445 - lambda_14_loss: 0.8752\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6401 - activation_14_loss: 0.6401 - lambda_14_loss: 0.8745\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6385 - activation_14_loss: 0.6385 - lambda_14_loss: 0.8679\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6385 - activation_14_loss: 0.6385 - lambda_14_loss: 0.8704\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6394 - activation_14_loss: 0.6394 - lambda_14_loss: 0.8772\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6359 - activation_14_loss: 0.6359 - lambda_14_loss: 0.8880\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6341 - activation_14_loss: 0.6341 - lambda_14_loss: 0.8759\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6360 - activation_14_loss: 0.6360 - lambda_14_loss: 0.8745\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6319 - activation_14_loss: 0.6319 - lambda_14_loss: 0.8817\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6320 - activation_14_loss: 0.6320 - lambda_14_loss: 0.8955\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6306 - activation_14_loss: 0.6306 - lambda_14_loss: 0.8774\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6328 - activation_14_loss: 0.6328 - lambda_14_loss: 0.8771\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6268 - activation_14_loss: 0.6267 - lambda_14_loss: 0.8798\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 257s 10s/step - loss: 0.6337 - activation_14_loss: 0.6337 - lambda_14_loss: 0.8684\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 256s 9s/step - loss: 0.6309 - activation_14_loss: 0.6308 - lambda_14_loss: 0.8735\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 256s 9s/step - loss: 0.6277 - activation_14_loss: 0.6277 - lambda_14_loss: 0.8690\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 259s 10s/step - loss: 0.6258 - activation_14_loss: 0.6257 - lambda_14_loss: 0.8809\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 256s 9s/step - loss: 0.6310 - activation_14_loss: 0.6310 - lambda_14_loss: 0.8718\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 258s 10s/step - loss: 0.6270 - activation_14_loss: 0.6270 - lambda_14_loss: 0.8574\n",
      "14/14 [==============================] - 26s 2s/step\n",
      "cpu_time:\n",
      "78655.921875\n",
      "f1:\n",
      "(array([0.74454829, 0.5631068 ]), array([0.8415493 , 0.41428571]), array([0.79008264, 0.47736626]), array([284, 140], dtype=int64))\n",
      "auc:\n",
      "0.6279175050301812\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
      " 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1162.  535.]\n",
      "[300. 124.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_16/Sigmoid:0', description=\"created by layer 'activation_16'\")\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 192, 22)     88          ['input_22[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_20[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 30)           120         ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_11 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_5[0][0]']     \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 30)          120         ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_11 (AttLayer)        (None, 400)          80400       ['bidirectional_11[0][0]']       \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 30)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 192, 1)       400         ['bidirectional_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 430)          0           ['att_layer_11[0][0]',           \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 192, 1)      4           ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 2)            860         ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 192, 1)       0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 2)           8           ['dense_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 193, 1)       0           ['input_23[0][0]',               \n",
      "                                                                  'activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 2)            0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_17 (Lambda)             (None, 192, 1)       0           ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 250s 9s/step - loss: 0.7472 - activation_17_loss: 0.7472 - lambda_17_loss: 0.7923\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.7028 - activation_17_loss: 0.7027 - lambda_17_loss: 0.7567\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 241s 9s/step - loss: 0.6855 - activation_17_loss: 0.6855 - lambda_17_loss: 0.7454\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6717 - activation_17_loss: 0.6717 - lambda_17_loss: 0.7602\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 239s 9s/step - loss: 0.6690 - activation_17_loss: 0.6690 - lambda_17_loss: 0.7721\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6699 - activation_17_loss: 0.6699 - lambda_17_loss: 0.7750\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6613 - activation_17_loss: 0.6613 - lambda_17_loss: 0.7936\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6602 - activation_17_loss: 0.6602 - lambda_17_loss: 0.8007\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6601 - activation_17_loss: 0.6601 - lambda_17_loss: 0.8062\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6573 - activation_17_loss: 0.6573 - lambda_17_loss: 0.8032\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6564 - activation_17_loss: 0.6564 - lambda_17_loss: 0.7983\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6519 - activation_17_loss: 0.6519 - lambda_17_loss: 0.8007\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6530 - activation_17_loss: 0.6530 - lambda_17_loss: 0.8008\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6561 - activation_17_loss: 0.6561 - lambda_17_loss: 0.8181\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6534 - activation_17_loss: 0.6534 - lambda_17_loss: 0.8221\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6522 - activation_17_loss: 0.6522 - lambda_17_loss: 0.8170\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6493 - activation_17_loss: 0.6492 - lambda_17_loss: 0.8329\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6522 - activation_17_loss: 0.6522 - lambda_17_loss: 0.8088\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6501 - activation_17_loss: 0.6501 - lambda_17_loss: 0.8215\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6545 - activation_17_loss: 0.6544 - lambda_17_loss: 0.8131\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 237s 9s/step - loss: 0.6513 - activation_17_loss: 0.6512 - lambda_17_loss: 0.8242\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 238s 9s/step - loss: 0.6449 - activation_17_loss: 0.6449 - lambda_17_loss: 0.8307\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 239s 9s/step - loss: 0.6474 - activation_17_loss: 0.6474 - lambda_17_loss: 0.8176\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 239s 9s/step - loss: 0.6515 - activation_17_loss: 0.6515 - lambda_17_loss: 0.8196\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 240s 9s/step - loss: 0.6487 - activation_17_loss: 0.6486 - lambda_17_loss: 0.8140\n",
      "14/14 [==============================] - 27s 2s/step\n",
      "cpu_time:\n",
      "79945.8125\n",
      "f1:\n",
      "(array([0.765625  , 0.47115385]), array([0.81666667, 0.39516129]), array([0.79032258, 0.42982456]), array([300, 124], dtype=int64))\n",
      "auc:\n",
      "0.6059139784946236\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
      " 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0\n",
      " 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1177.  520.]\n",
      "[285. 139.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_19/Sigmoid:0', description=\"created by layer 'activation_19'\")\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 192, 22)     88          ['input_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_24[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 30)           120         ['input_28[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_13 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_6[0][0]']     \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 30)          120         ['dense_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_13 (AttLayer)        (None, 400)          80400       ['bidirectional_13[0][0]']       \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 30)           0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 192, 1)       400         ['bidirectional_13[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 430)          0           ['att_layer_13[0][0]',           \n",
      "                                                                  'activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 192, 1)      4           ['dense_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 2)            860         ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " input_27 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 192, 1)       0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 2)           8           ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 193, 1)       0           ['input_27[0][0]',               \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 2)            0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_20 (Lambda)             (None, 192, 1)       0           ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 316s 11s/step - loss: 0.7403 - activation_20_loss: 0.7402 - lambda_20_loss: 0.8120\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 298s 11s/step - loss: 0.6976 - activation_20_loss: 0.6976 - lambda_20_loss: 0.8236\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 300s 11s/step - loss: 0.6824 - activation_20_loss: 0.6824 - lambda_20_loss: 0.7965\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 296s 11s/step - loss: 0.6743 - activation_20_loss: 0.6743 - lambda_20_loss: 0.7922\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 298s 11s/step - loss: 0.6721 - activation_20_loss: 0.6721 - lambda_20_loss: 0.7949\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 296s 11s/step - loss: 0.6719 - activation_20_loss: 0.6719 - lambda_20_loss: 0.7845\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6637 - activation_20_loss: 0.6637 - lambda_20_loss: 0.7897\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6683 - activation_20_loss: 0.6683 - lambda_20_loss: 0.7717\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 300s 11s/step - loss: 0.6647 - activation_20_loss: 0.6647 - lambda_20_loss: 0.7753\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 299s 11s/step - loss: 0.6623 - activation_20_loss: 0.6623 - lambda_20_loss: 0.7810\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6628 - activation_20_loss: 0.6628 - lambda_20_loss: 0.7851\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6587 - activation_20_loss: 0.6587 - lambda_20_loss: 0.7765\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 299s 11s/step - loss: 0.6649 - activation_20_loss: 0.6649 - lambda_20_loss: 0.7723\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 298s 11s/step - loss: 0.6577 - activation_20_loss: 0.6577 - lambda_20_loss: 0.7686\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 298s 11s/step - loss: 0.6593 - activation_20_loss: 0.6593 - lambda_20_loss: 0.7764\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 298s 11s/step - loss: 0.6607 - activation_20_loss: 0.6607 - lambda_20_loss: 0.7746\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 299s 11s/step - loss: 0.6598 - activation_20_loss: 0.6598 - lambda_20_loss: 0.7716\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6584 - activation_20_loss: 0.6584 - lambda_20_loss: 0.7862\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6546 - activation_20_loss: 0.6546 - lambda_20_loss: 0.7743\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.6564 - activation_20_loss: 0.6564 - lambda_20_loss: 0.7966\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 296s 11s/step - loss: 0.6594 - activation_20_loss: 0.6594 - lambda_20_loss: 0.7698\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 295s 11s/step - loss: 0.6581 - activation_20_loss: 0.6581 - lambda_20_loss: 0.7687\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 296s 11s/step - loss: 0.6583 - activation_20_loss: 0.6583 - lambda_20_loss: 0.7746\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 295s 11s/step - loss: 0.6604 - activation_20_loss: 0.6604 - lambda_20_loss: 0.7797\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 294s 11s/step - loss: 0.6606 - activation_20_loss: 0.6606 - lambda_20_loss: 0.7745\n",
      "14/14 [==============================] - 32s 2s/step\n",
      "cpu_time:\n",
      "91238.09375\n",
      "f1:\n",
      "(array([0.75 , 0.525]), array([0.8       , 0.45323741]), array([0.77419355, 0.48648649]), array([285, 139], dtype=int64))\n",
      "auc:\n",
      "0.6266187050359713\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1\n",
      " 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1164.  533.]\n",
      "[298. 126.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_22/Sigmoid:0', description=\"created by layer 'activation_22'\")\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 192, 22)     88          ['input_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_32 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_28[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 30)           120         ['input_32[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_15 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_7[0][0]']     \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 30)          120         ['dense_22[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_15 (AttLayer)        (None, 400)          80400       ['bidirectional_15[0][0]']       \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 30)           0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 192, 1)       400         ['bidirectional_15[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 430)          0           ['att_layer_15[0][0]',           \n",
      "                                                                  'activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 192, 1)      4           ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 2)            860         ['concatenate_23[0][0]']         \n",
      "                                                                                                  \n",
      " input_31 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 192, 1)       0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 2)           8           ['dense_23[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 193, 1)       0           ['input_31[0][0]',               \n",
      "                                                                  'activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 2)            0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_23 (Lambda)             (None, 192, 1)       0           ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 264s 9s/step - loss: 0.7241 - activation_23_loss: 0.7240 - lambda_23_loss: 0.8088\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.7004 - activation_23_loss: 0.7004 - lambda_23_loss: 0.7276\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6799 - activation_23_loss: 0.6799 - lambda_23_loss: 0.7935\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6703 - activation_23_loss: 0.6703 - lambda_23_loss: 0.8416\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6693 - activation_23_loss: 0.6693 - lambda_23_loss: 0.7909\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6624 - activation_23_loss: 0.6624 - lambda_23_loss: 0.7497\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6639 - activation_23_loss: 0.6639 - lambda_23_loss: 0.7473\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6584 - activation_23_loss: 0.6584 - lambda_23_loss: 0.7310\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6587 - activation_23_loss: 0.6586 - lambda_23_loss: 0.7135\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6598 - activation_23_loss: 0.6598 - lambda_23_loss: 0.7048\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6553 - activation_23_loss: 0.6553 - lambda_23_loss: 0.7057\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6545 - activation_23_loss: 0.6545 - lambda_23_loss: 0.6958\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6581 - activation_23_loss: 0.6581 - lambda_23_loss: 0.6911\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6528 - activation_23_loss: 0.6528 - lambda_23_loss: 0.6847\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6520 - activation_23_loss: 0.6520 - lambda_23_loss: 0.6868\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6510 - activation_23_loss: 0.6509 - lambda_23_loss: 0.6827\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 254s 9s/step - loss: 0.6539 - activation_23_loss: 0.6539 - lambda_23_loss: 0.6703\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6499 - activation_23_loss: 0.6499 - lambda_23_loss: 0.6823\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 255s 9s/step - loss: 0.6469 - activation_23_loss: 0.6469 - lambda_23_loss: 0.6721\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 257s 10s/step - loss: 0.6481 - activation_23_loss: 0.6481 - lambda_23_loss: 0.6717\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 260s 10s/step - loss: 0.6487 - activation_23_loss: 0.6487 - lambda_23_loss: 0.6728\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 259s 10s/step - loss: 0.6460 - activation_23_loss: 0.6460 - lambda_23_loss: 0.6715\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 259s 10s/step - loss: 0.6463 - activation_23_loss: 0.6463 - lambda_23_loss: 0.6710\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 261s 10s/step - loss: 0.6416 - activation_23_loss: 0.6416 - lambda_23_loss: 0.6685\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 259s 10s/step - loss: 0.6430 - activation_23_loss: 0.6430 - lambda_23_loss: 0.6696\n",
      "14/14 [==============================] - 28s 2s/step\n",
      "cpu_time:\n",
      "82165.203125\n",
      "f1:\n",
      "(array([0.75138122, 0.58064516]), array([0.91275168, 0.28571429]), array([0.82424242, 0.38297872]), array([298, 126], dtype=int64))\n",
      "auc:\n",
      "0.5992329817833173\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0\n",
      " 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1173.  524.]\n",
      "[289. 135.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_25/Sigmoid:0', description=\"created by layer 'activation_25'\")\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 192, 22)     88          ['input_34[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_32[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 30)           120         ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_17 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_8[0][0]']     \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 30)          120         ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_17 (AttLayer)        (None, 400)          80400       ['bidirectional_17[0][0]']       \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 30)           0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 192, 1)       400         ['bidirectional_17[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 430)          0           ['att_layer_17[0][0]',           \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 192, 1)      4           ['dense_24[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 2)            860         ['concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 192, 1)       0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 2)           8           ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 193, 1)       0           ['input_35[0][0]',               \n",
      "                                                                  'activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 2)            0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_26 (Lambda)             (None, 192, 1)       0           ['concatenate_25[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 302s 11s/step - loss: 0.7874 - activation_26_loss: 0.7873 - lambda_26_loss: 0.9382\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 287s 11s/step - loss: 0.7543 - activation_26_loss: 0.7543 - lambda_26_loss: 1.0019\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 288s 11s/step - loss: 0.7348 - activation_26_loss: 0.7348 - lambda_26_loss: 1.0480\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 287s 11s/step - loss: 0.7165 - activation_26_loss: 0.7164 - lambda_26_loss: 1.0813\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 287s 11s/step - loss: 0.6972 - activation_26_loss: 0.6972 - lambda_26_loss: 1.0834\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 285s 11s/step - loss: 0.6897 - activation_26_loss: 0.6897 - lambda_26_loss: 1.0750\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 284s 11s/step - loss: 0.6800 - activation_26_loss: 0.6800 - lambda_26_loss: 1.0701\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 284s 11s/step - loss: 0.6790 - activation_26_loss: 0.6789 - lambda_26_loss: 1.0616\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 282s 10s/step - loss: 0.6722 - activation_26_loss: 0.6722 - lambda_26_loss: 1.0531\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6733 - activation_26_loss: 0.6733 - lambda_26_loss: 1.0576\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6734 - activation_26_loss: 0.6734 - lambda_26_loss: 1.0610\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6659 - activation_26_loss: 0.6659 - lambda_26_loss: 1.0507\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6676 - activation_26_loss: 0.6675 - lambda_26_loss: 1.0434\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 280s 10s/step - loss: 0.6674 - activation_26_loss: 0.6674 - lambda_26_loss: 1.0382\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6662 - activation_26_loss: 0.6662 - lambda_26_loss: 1.0390\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6663 - activation_26_loss: 0.6663 - lambda_26_loss: 1.0360\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 280s 10s/step - loss: 0.6693 - activation_26_loss: 0.6693 - lambda_26_loss: 1.0276\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 280s 10s/step - loss: 0.6677 - activation_26_loss: 0.6677 - lambda_26_loss: 1.0300\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6674 - activation_26_loss: 0.6674 - lambda_26_loss: 1.0295\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6615 - activation_26_loss: 0.6615 - lambda_26_loss: 1.0312\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6627 - activation_26_loss: 0.6626 - lambda_26_loss: 1.0186\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6616 - activation_26_loss: 0.6616 - lambda_26_loss: 1.0212\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 280s 10s/step - loss: 0.6598 - activation_26_loss: 0.6598 - lambda_26_loss: 1.0181\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 282s 10s/step - loss: 0.6644 - activation_26_loss: 0.6644 - lambda_26_loss: 1.0233\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 283s 10s/step - loss: 0.6605 - activation_26_loss: 0.6605 - lambda_26_loss: 1.0134\n",
      "14/14 [==============================] - 29s 2s/step\n",
      "cpu_time:\n",
      "88169.8125\n",
      "f1:\n",
      "(array([0.75539568, 0.45890411]), array([0.7266436, 0.4962963]), array([0.74074074, 0.47686833]), array([289, 135], dtype=int64))\n",
      "auc:\n",
      "0.6114699474561066\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0\n",
      " 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1170.  527.]\n",
      "[292. 132.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_28/Sigmoid:0', description=\"created by layer 'activation_28'\")\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 192, 22)     88          ['input_38[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDistri  (None, 192, 402)    4006800     ['batch_normalization_36[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 30)           120         ['input_40[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_19 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_9[0][0]']     \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 30)          120         ['dense_28[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_19 (AttLayer)        (None, 400)          80400       ['bidirectional_19[0][0]']       \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 30)           0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 192, 1)       400         ['bidirectional_19[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 430)          0           ['att_layer_19[0][0]',           \n",
      "                                                                  'activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 192, 1)      4           ['dense_27[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 2)            860         ['concatenate_29[0][0]']         \n",
      "                                                                                                  \n",
      " input_39 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 192, 1)       0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 2)           8           ['dense_29[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 193, 1)       0           ['input_39[0][0]',               \n",
      "                                                                  'activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 2)            0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_29 (Lambda)             (None, 192, 1)       0           ['concatenate_28[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.7633 - activation_29_loss: 0.7633 - lambda_29_loss: 0.9439\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.7236 - activation_29_loss: 0.7235 - lambda_29_loss: 0.8458\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 297s 11s/step - loss: 0.7114 - activation_29_loss: 0.7114 - lambda_29_loss: 0.8274\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 291s 11s/step - loss: 0.7120 - activation_29_loss: 0.7120 - lambda_29_loss: 0.8170\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 291s 11s/step - loss: 0.7085 - activation_29_loss: 0.7085 - lambda_29_loss: 0.7962\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 283s 10s/step - loss: 0.7055 - activation_29_loss: 0.7055 - lambda_29_loss: 0.7886\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 301s 11s/step - loss: 0.7087 - activation_29_loss: 0.7086 - lambda_29_loss: 0.7918\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 283s 10s/step - loss: 0.6999 - activation_29_loss: 0.6999 - lambda_29_loss: 0.7701\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6988 - activation_29_loss: 0.6988 - lambda_29_loss: 0.7580\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6969 - activation_29_loss: 0.6969 - lambda_29_loss: 0.7758\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6977 - activation_29_loss: 0.6977 - lambda_29_loss: 0.7532\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6932 - activation_29_loss: 0.6932 - lambda_29_loss: 0.7532\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6969 - activation_29_loss: 0.6969 - lambda_29_loss: 0.7470\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6941 - activation_29_loss: 0.6941 - lambda_29_loss: 0.7382\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6922 - activation_29_loss: 0.6922 - lambda_29_loss: 0.7424\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 269s 10s/step - loss: 0.6932 - activation_29_loss: 0.6932 - lambda_29_loss: 0.7332\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 283s 10s/step - loss: 0.6949 - activation_29_loss: 0.6949 - lambda_29_loss: 0.7390\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 291s 11s/step - loss: 0.6928 - activation_29_loss: 0.6927 - lambda_29_loss: 0.7306\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 286s 11s/step - loss: 0.6837 - activation_29_loss: 0.6837 - lambda_29_loss: 0.7226\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 274s 10s/step - loss: 0.6930 - activation_29_loss: 0.6930 - lambda_29_loss: 0.7352\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 281s 10s/step - loss: 0.6840 - activation_29_loss: 0.6840 - lambda_29_loss: 0.7037\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 285s 11s/step - loss: 0.6816 - activation_29_loss: 0.6816 - lambda_29_loss: 0.7219\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 283s 10s/step - loss: 0.6848 - activation_29_loss: 0.6848 - lambda_29_loss: 0.7112\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 282s 10s/step - loss: 0.6805 - activation_29_loss: 0.6804 - lambda_29_loss: 0.7180\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 270s 10s/step - loss: 0.6869 - activation_29_loss: 0.6869 - lambda_29_loss: 0.7086\n",
      "14/14 [==============================] - 30s 2s/step\n",
      "cpu_time:\n",
      "86599.09375\n",
      "f1:\n",
      "(array([0.72289157, 0.43478261]), array([0.82191781, 0.3030303 ]), array([0.76923077, 0.35714286]), array([292, 132], dtype=int64))\n",
      "auc:\n",
      "0.5624740556247405\n",
      "(2121, 192, 21)\n",
      "(2121, 192)\n",
      "(2121, 192, 1)\n",
      "(2121, 4)\n",
      "y_single:  [1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0]\n",
      "Number of positive and negative posts in training and test set\n",
      "[1172.  525.]\n",
      "[290. 134.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_embedding: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), name='activation_31/Sigmoid:0', description=\"created by layer 'activation_31'\")\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 192, 22)     88          ['input_42[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " input_44 (InputLayer)          [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_10 (TimeDistr  (None, 192, 402)    4006800     ['batch_normalization_40[0][0]'] \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 30)           120         ['input_44[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_21 (Bidirectiona  (None, 192, 400)    724800      ['time_distributed_10[0][0]']    \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 30)          120         ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_21 (AttLayer)        (None, 400)          80400       ['bidirectional_21[0][0]']       \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 30)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 192, 1)       400         ['bidirectional_21[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 430)          0           ['att_layer_21[0][0]',           \n",
      "                                                                  'activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 192, 1)      4           ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 2)            860         ['concatenate_32[0][0]']         \n",
      "                                                                                                  \n",
      " input_43 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 192, 1)       0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 2)           8           ['dense_32[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 193, 1)       0           ['input_43[0][0]',               \n",
      "                                                                  'activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 2)            0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_32 (Lambda)             (None, 192, 1)       0           ['concatenate_31[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,813,600\n",
      "Trainable params: 4,813,490\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 331s 12s/step - loss: 0.7558 - activation_32_loss: 0.7558 - lambda_32_loss: 1.6356\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6969 - activation_32_loss: 0.6968 - lambda_32_loss: 1.6707\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 314s 12s/step - loss: 0.6763 - activation_32_loss: 0.6763 - lambda_32_loss: 1.6347\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 313s 12s/step - loss: 0.6684 - activation_32_loss: 0.6684 - lambda_32_loss: 1.5929\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 314s 12s/step - loss: 0.6698 - activation_32_loss: 0.6698 - lambda_32_loss: 1.5553\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6734 - activation_32_loss: 0.6734 - lambda_32_loss: 1.5219\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 314s 12s/step - loss: 0.6695 - activation_32_loss: 0.6695 - lambda_32_loss: 1.4952\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 318s 12s/step - loss: 0.6684 - activation_32_loss: 0.6684 - lambda_32_loss: 1.4778\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6677 - activation_32_loss: 0.6677 - lambda_32_loss: 1.4596\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6597 - activation_32_loss: 0.6597 - lambda_32_loss: 1.4358\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6698 - activation_32_loss: 0.6698 - lambda_32_loss: 1.4259\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 317s 12s/step - loss: 0.6670 - activation_32_loss: 0.6670 - lambda_32_loss: 1.4154\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6624 - activation_32_loss: 0.6624 - lambda_32_loss: 1.3987\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6637 - activation_32_loss: 0.6636 - lambda_32_loss: 1.3941\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6660 - activation_32_loss: 0.6659 - lambda_32_loss: 1.3864\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6654 - activation_32_loss: 0.6654 - lambda_32_loss: 1.3836\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 317s 12s/step - loss: 0.6650 - activation_32_loss: 0.6650 - lambda_32_loss: 1.3820\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6656 - activation_32_loss: 0.6656 - lambda_32_loss: 1.3739\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 317s 12s/step - loss: 0.6656 - activation_32_loss: 0.6656 - lambda_32_loss: 1.3636\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6573 - activation_32_loss: 0.6573 - lambda_32_loss: 1.3657\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6604 - activation_32_loss: 0.6604 - lambda_32_loss: 1.3627\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6569 - activation_32_loss: 0.6568 - lambda_32_loss: 1.3553\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 316s 12s/step - loss: 0.6629 - activation_32_loss: 0.6629 - lambda_32_loss: 1.3483\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 315s 12s/step - loss: 0.6606 - activation_32_loss: 0.6606 - lambda_32_loss: 1.3475\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 317s 12s/step - loss: 0.6622 - activation_32_loss: 0.6622 - lambda_32_loss: 1.3516\n",
      "14/14 [==============================] - 31s 2s/step\n",
      "cpu_time:\n",
      "96891.125\n",
      "f1:\n",
      "(array([0.75747508, 0.49593496]), array([0.7862069 , 0.45522388]), array([0.7715736 , 0.47470817]), array([290, 134], dtype=int64))\n",
      "auc:\n",
      "0.6207153885743696\n",
      "[0.5978673986486486, 0.6130877808402238, 0.6026730079605279, 0.6279175050301812, 0.6059139784946236, 0.6266187050359713, 0.5992329817833173, 0.6114699474561066, 0.5624740556247405, 0.6207153885743696]\n",
      "[0.4267782426778243, 0.45283018867924524, 0.4351464435146444, 0.4773662551440329, 0.4298245614035088, 0.4864864864864865, 0.3829787234042553, 0.47686832740213525, 0.35714285714285715, 0.4747081712062256]\n",
      "[0.4594594594594595, 0.40540540540540543, 0.5098039215686274, 0.5631067961165048, 0.47115384615384615, 0.525, 0.5806451612903226, 0.4589041095890411, 0.43478260869565216, 0.4959349593495935]\n",
      "[0.3984375, 0.5128205128205128, 0.3795620437956204, 0.4142857142857143, 0.3951612903225806, 0.45323741007194246, 0.2857142857142857, 0.4962962962962963, 0.30303030303030304, 0.4552238805970149]\n",
      "[70823.203125, 71433.125, 77328.421875, 78655.921875, 79945.8125, 91238.09375, 82165.203125, 88169.8125, 86599.09375, 96891.125]\n",
      "TIME 82324.98125 7994.268046980453\n",
      "AUC 0.606797074944871 0.017925875745721572\n",
      "f1 0.44001302570612155 0.04087103710825699\n",
      "precision 0.49041962676284523 0.052604058168907904\n",
      "recall 0.40937692369342704 0.07079411660410341\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    start_time = process_time()\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data1 = data[indices]\n",
    "    print(data1.shape)\n",
    "    labels1 = labels[indices]\n",
    "    single_label1=single_label[indices]\n",
    "    timeInfo1=timeInfo[indices]\n",
    "    print(timeInfo1.shape)\n",
    "    timeInfo1=timeInfo1.reshape((2121,MAX_SENTS,1))\n",
    "    print(timeInfo1.shape)\n",
    "    data1=np.dstack((data1, timeInfo1))\n",
    "    postInfo1=postInfo[indices]\n",
    "    print(postInfo1.shape)\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data1.shape[0])\n",
    "    zeros=np.zeros(2121)\n",
    "    zeros=zeros.reshape((2121,1,1))\n",
    "\n",
    "    x_train = data1[:-nb_validation_samples]\n",
    "    y_train = labels1[:-nb_validation_samples]\n",
    "    zeros_train=zeros[:-nb_validation_samples]\n",
    "    time_train=timeInfo1[:-nb_validation_samples]\n",
    "    post_train=postInfo1[:-nb_validation_samples]\n",
    "    median_value1 = np.median(post_train)\n",
    "    post_train = np.where(np.isnan(post_train), 0, post_train)\n",
    "    x_val = data1[-nb_validation_samples:]\n",
    "    y_val = labels1[-nb_validation_samples:]\n",
    "    zeros_test=zeros[-nb_validation_samples:]\n",
    "    time_test=timeInfo1[-nb_validation_samples:]\n",
    "    post_test=postInfo1[-nb_validation_samples:]\n",
    "    median_value2 = np.median(post_test)\n",
    "    post_test = np.where(np.isnan(post_test), 0, post_test)\n",
    "    y_single=single_label1[-nb_validation_samples:]\n",
    "    print(\"y_single: \",y_single)\n",
    "\n",
    "    # break\n",
    "    print('Number of positive and negative posts in training and test set')\n",
    "    print (y_train.sum(axis=0))\n",
    "    print (y_val.sum(axis=0))\n",
    "\n",
    "    # building Hierachical Attention network\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                POST_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True)\n",
    "\n",
    "    all_input = Input(shape=(MAX_SENT_LENGTH+2,))\n",
    "    sentence_input=crop(1, 0, MAX_SENT_LENGTH)(all_input)##slice\n",
    "    time_input=crop(1, MAX_SENT_LENGTH, MAX_SENT_LENGTH+2)(all_input)##slice\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    l_lstm = Bidirectional(GRU(200, return_sequences=True))(embedded_sequences)\n",
    "    l_att = AttLayer(200)(l_lstm)####(?,200)\n",
    "    #time_embedding=Dense(TIME_DIM,activation='sigmoid')(time_input)\n",
    "    merged_output=Concatenate()([l_att,time_input])###text+time information\n",
    "    sentEncoder = Model(all_input, merged_output)\n",
    "    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH+2))\n",
    "    norm_review_input = BatchNormalization()(review_input)\n",
    "    review_encoder =TimeDistributed(sentEncoder)(norm_review_input)  # <==\n",
    "\n",
    "    l_lstm_sent = Bidirectional(GRU(200, return_sequences=True))(review_encoder)\n",
    "    #l_lstm_sent = Bidirectional(GRU(200, return_sequences=True))(review_input)\n",
    "    #pred_time=Dense(1,activation='relu')(l_lstm_sent)\n",
    "    fully_sent=Dense(1,use_bias=False)(l_lstm_sent)\n",
    "    norm_fullysent=BatchNormalization()(fully_sent)\n",
    "    pred_time=Activation(activation='linear')(norm_fullysent)\n",
    "\n",
    "    zero_input=Input(shape=(1,1))\n",
    "    shift_predtime=Concatenate(axis=1)([zero_input,pred_time])\n",
    "    shift_predtime=crop(1, 0, MAX_SENTS)(shift_predtime)\n",
    "    l_att_sent = AttLayer(200)(l_lstm_sent)\n",
    "\n",
    "    ###embed the #likes, shares\n",
    "    # post_input=Input(shape=(4,))\n",
    "    # post_input = Input(shape=(None,4))    # <==\n",
    "    post_input = Input(shape=(4,))    # <==\n",
    "    #post_embedding = Dense(INFO_DIM, activation='sigmoid')(post_input)\n",
    "    fully_post=Dense(INFO_DIM,use_bias=False)(post_input)\n",
    "    norm_fullypost=BatchNormalization()(fully_post)\n",
    "    post_embedding=Activation(activation='sigmoid')(norm_fullypost)\n",
    "    # print(\"l_att_sent: \")\n",
    "    # print(l_att_sent)\n",
    "    print(\"post_embedding: \")\n",
    "    print(post_embedding)\n",
    "    x = concatenate([l_att_sent,post_embedding])###merge the document level vectro with the additional embedded features such as #likes\n",
    "    fully_review=Dense(2,use_bias=False)(x)\n",
    "    norm_fullyreview=BatchNormalization()(fully_review)\n",
    "    preds=Activation(activation='softmax')(norm_fullyreview)\n",
    "\n",
    "    # rmsprop = optimizers.adam(lr=0.001)\n",
    "    rmsprop = optimizers.Adam(learning_rate=0.001,decay=0.99)   # <==\n",
    "\n",
    "    # print(\"review_input: \", review_input)\n",
    "    # print(\"post_input: \", post_input)\n",
    "    # print(\"zero_input: \", zero_input)\n",
    "\n",
    "    model = Model(inputs=[review_input,post_input,zero_input], outputs=[preds,shift_predtime])\n",
    "    print(model.summary())\n",
    "    model.compile(loss=['binary_crossentropy','mse'],loss_weights=[1,0.00002],\n",
    "                  optimizer=rmsprop)\n",
    "    #filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "    #checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    #callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\n",
    "\n",
    "    print(\"model fitting - Hierachical attention network for cyberbullying detection\")\n",
    "\n",
    "    model.fit([x_train,post_train,zeros_train], [y_train,time_train], batch_size=64,\n",
    "              epochs=25,verbose=1)\n",
    "    yp = model.predict([x_val,post_test,zeros_test], verbose=1)\n",
    "    ypreds=yp[0]\n",
    "    ypreds = np.argmax(ypreds, axis=1)\n",
    "    #print y_single\n",
    "    #print ypred\n",
    "    f1=precision_recall_fscore_support(y_single.astype(int), ypreds)   # <==\n",
    "    auc=roc_auc_score(y_single.astype('int'), ypreds)  #<==\n",
    "    f1 = precision_recall_fscore_support(y_single.astype(int), ypreds)  # <==\n",
    "    auc = roc_auc_score(y_single.astype('int'), ypreds)  # <== category\n",
    "    end_time = process_time()\n",
    "    cpu_time = end_time - start_time\n",
    "    print(\"cpu_time:\")\n",
    "    print(cpu_time)\n",
    "    print(\"f1:\")\n",
    "    print(f1)\n",
    "    print(\"auc:\")\n",
    "    print(auc)\n",
    "    HAN_TIME.append(cpu_time)\n",
    "    HAN_AUC.append(auc)\n",
    "    HAN_f1.append(f1[2][1])\n",
    "    HAN_reca.append(f1[1][1])\n",
    "    HAN_pre.append(f1[0][1])\n",
    "\n",
    "    #for t-sne visualization\n",
    "    # if j==0:\n",
    "    #     a=model.layers\n",
    "    #     get_representations_test = K.function([model.layers[0].input,model.layers[1].input,model.layers[12].input], [model.layers[6].output])\n",
    "    #     representations_test = get_representations_test([x_val,post_test,zeros_test])[0]\n",
    "    #     representation_dict = {\n",
    "    #         'representations': representations_test,\n",
    "    #         'labels': y_single\n",
    "    #     }\n",
    "    #\n",
    "    #     with open('HANCD_Tem_results.pickle', 'wb') as handle:\n",
    "    #         pickle.dump(representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # K.clear_session()\n",
    "\n",
    "print(HAN_AUC)\n",
    "print(HAN_f1)\n",
    "print(HAN_pre)\n",
    "print(HAN_reca)\n",
    "print(HAN_TIME)\n",
    "print (\"TIME\",np.mean(HAN_TIME), np.std(HAN_TIME))\n",
    "print (\"AUC\",np.mean(HAN_AUC), np.std(HAN_AUC))\n",
    "print (\"f1\", np.mean(HAN_f1), np.std(HAN_f1))\n",
    "print (\"precision\",np.mean(HAN_pre), np.std(HAN_pre))\n",
    "print (\"recall\", np.mean(HAN_reca), np.std(HAN_reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61ea40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa7350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
