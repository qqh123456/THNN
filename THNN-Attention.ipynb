{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57105e2d-d44e-4d4f-af27-43ff58f16a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:45:30.368798Z",
     "iopub.status.busy": "2023-05-28T03:45:30.368359Z",
     "iopub.status.idle": "2023-05-28T03:45:32.652835Z",
     "shell.execute_reply": "2023-05-28T03:45:32.652409Z",
     "shell.execute_reply.started": "2023-05-28T03:45:30.368762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Concatenate, BatchNormalization, Activation,LayerNormalization\n",
    "from keras.layers import Lambda, Embedding, GRU, Bidirectional, TimeDistributed, concatenate\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from word2vecReader import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "# from tensorflow.python.keras.optimizers import adam_v2\n",
    "# from tensorflow.python.keras.optimizers import rmsprop_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from  keras import layers\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07ca67c-4ad5-4cf8-9495-47eb398d6b68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:45:56.515661Z",
     "iopub.status.busy": "2023-05-28T03:45:56.515192Z",
     "iopub.status.idle": "2023-05-28T03:45:56.523048Z",
     "shell.execute_reply": "2023-05-28T03:45:56.521792Z",
     "shell.execute_reply.started": "2023-05-28T03:45:56.515623Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255013ef-c12c-4ce4-9c62-4a62726874e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:45:57.172764Z",
     "iopub.status.busy": "2023-05-28T03:45:57.171992Z",
     "iopub.status.idle": "2023-05-28T03:45:57.189493Z",
     "shell.execute_reply": "2023-05-28T03:45:57.187896Z",
     "shell.execute_reply.started": "2023-05-28T03:45:57.172704Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(1)\n",
    "np.random.seed(0)\n",
    "MAX_SENT_LENGTH = 20  #number of words in a sentence\n",
    "MAX_NB_WORDS = 20000\n",
    "POST_DIM = 400\n",
    "INFO_DIM = 30\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209ddfce-f0ed-4f92-b963-ad562cf97990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:45:57.850476Z",
     "iopub.status.busy": "2023-05-28T03:45:57.849987Z",
     "iopub.status.idle": "2023-05-28T03:45:57.892066Z",
     "shell.execute_reply": "2023-05-28T03:45:57.890837Z",
     "shell.execute_reply.started": "2023-05-28T03:45:57.850436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##slice tensor function in keras\n",
    "def crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "\n",
    "    return Lambda(func)\n",
    "\n",
    "\n",
    "def myFunc(x):\n",
    "    if \"empety\" in x:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = string.strip().lower()\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_words = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def find_str(s, char):\n",
    "    index = 0\n",
    "\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index + len(char)] == char:\n",
    "                    return index\n",
    "\n",
    "            index += 1\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weightss = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    \n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''Transformer的Encoder部分'''\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "'''Transformer输入的编码层'''\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3a960f-cae4-4327-8b4b-ad2f3afe8c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:45:58.899493Z",
     "iopub.status.busy": "2023-05-28T03:45:58.899032Z",
     "iopub.status.idle": "2023-05-28T03:46:16.233992Z",
     "shell.execute_reply": "2023-05-28T03:46:16.233514Z",
     "shell.execute_reply.started": "2023-05-28T03:45:58.899459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8009 unique tokens.\n",
      "Shape of data tensor: (2121, 192, 21)\n",
      "Shape of label tensor: (2121, 2)\n"
     ]
    }
   ],
   "source": [
    "with open('instagram.pickle', 'rb') as handle:\n",
    "    dictionary = pickle.load(handle)\n",
    "texts = dictionary['text']\n",
    "texts = texts.fillna(\"\")\n",
    "# texts=[text.encode('ascii') for text in texts]\n",
    "#print(texts)\n",
    "comments = dictionary['comments']\n",
    "#print(comments)\n",
    "timeInfo = dictionary['time']\n",
    "#print(timeInfo)\n",
    "postInfo = dictionary['post']\n",
    "#print(postInfo)\n",
    "\n",
    "#print(postInfo)\n",
    "labels = dictionary['labels']\n",
    "#print(labels)\n",
    "b = np.zeros([len(timeInfo), len(max(timeInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(timeInfo):\n",
    "    b[i][0:len(j)] = j\n",
    "timeInfo = b\n",
    "#print(b)\n",
    "time_size = len(np.unique(timeInfo))\n",
    "MAX_SENTS = len(timeInfo[0])  ####number of sentences\n",
    "\n",
    "c = np.zeros([len(postInfo), len(max(postInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(postInfo):\n",
    "    c[i][0:len(j)] = j\n",
    "median_value = np.median(c)\n",
    "c = np.where(c > 10000000,median_value , c)\n",
    "postInfo = c\n",
    "#print(postInfo)\n",
    "post_size = len(np.unique(postInfo))\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "#print(MAX_SENTS)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH+1), dtype='int32')\n",
    "#print(tokenizer.word_index['club'])\n",
    "\n",
    "for i, sentences in enumerate(comments):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for word in wordTokens:\n",
    "                #print(type(wordTokens[0]))\n",
    "                #print(\"'{}'\".format(word))\n",
    "                if k < MAX_SENT_LENGTH and word in tokenizer.word_index:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "#print(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "single_label = np.asarray(labels)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "###standardize\n",
    "transfer = StandardScaler()\n",
    "timeInfo = transfer.fit_transform(timeInfo)\n",
    "postInfo = transfer.fit_transform(postInfo)\n",
    "HAN_pre = []\n",
    "HAN_reca = []\n",
    "HAN_f1 = []\n",
    "HAN_AUC = []\n",
    "HAN_TIME = []\n",
    "embeddings_index = Word2Vec.load_word2vec_format(\"word2vec_twitter_model.bin\", binary=True, )  #\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, POST_DIM))\n",
    "outword_dic = dict()\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        new_vector = np.random.rand(POST_DIM, )\n",
    "        outword_dic.setdefault(word, new_vector)\n",
    "        embedding_matrix[i] = outword_dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023cf799-fe17-4d2e-bb33-f32a0c02d3b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:53:46.695241Z",
     "iopub.status.busy": "2023-05-28T03:53:46.695007Z",
     "iopub.status.idle": "2023-05-28T03:54:44.159831Z",
     "shell.execute_reply": "2023-05-28T03:54:44.159529Z",
     "shell.execute_reply.started": "2023-05-28T03:53:46.695233Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1170.  527.]\n",
      "[292. 132.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.7205 - activation_2_loss: 0.7205 - lambda_3_loss: 1.8160\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 119s 4s/step - loss: 0.6464 - activation_2_loss: 0.6463 - lambda_3_loss: 1.8590\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6318 - activation_2_loss: 0.6317 - lambda_3_loss: 1.8748\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6294 - activation_2_loss: 0.6294 - lambda_3_loss: 1.8775\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6189 - activation_2_loss: 0.6189 - lambda_3_loss: 1.8738\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6190 - activation_2_loss: 0.6190 - lambda_3_loss: 1.8758\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6165 - activation_2_loss: 0.6165 - lambda_3_loss: 1.8697\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6147 - activation_2_loss: 0.6147 - lambda_3_loss: 1.8655\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6103 - activation_2_loss: 0.6102 - lambda_3_loss: 1.8646\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6096 - activation_2_loss: 0.6096 - lambda_3_loss: 1.8616\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6109 - activation_2_loss: 0.6108 - lambda_3_loss: 1.8643\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6058 - activation_2_loss: 0.6058 - lambda_3_loss: 1.8647\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6069 - activation_2_loss: 0.6069 - lambda_3_loss: 1.8626\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6082 - activation_2_loss: 0.6082 - lambda_3_loss: 1.8577\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6098 - activation_2_loss: 0.6098 - lambda_3_loss: 1.8581\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6065 - activation_2_loss: 0.6064 - lambda_3_loss: 1.8617\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6005 - activation_2_loss: 0.6005 - lambda_3_loss: 1.8566\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6031 - activation_2_loss: 0.6030 - lambda_3_loss: 1.8497\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6007 - activation_2_loss: 0.6007 - lambda_3_loss: 1.8479\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6023 - activation_2_loss: 0.6023 - lambda_3_loss: 1.8466\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.5996 - activation_2_loss: 0.5995 - lambda_3_loss: 1.8506\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.5964 - activation_2_loss: 0.5964 - lambda_3_loss: 1.8422\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.5977 - activation_2_loss: 0.5977 - lambda_3_loss: 1.8410\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.5952 - activation_2_loss: 0.5951 - lambda_3_loss: 1.8436\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5980 - activation_2_loss: 0.5979 - lambda_3_loss: 1.8423\n",
      "14/14 [==============================] - 11s 710ms/step\n",
      "cpu_time:\n",
      "35058.3125\n",
      "f1:\n",
      "(array([0.7745098 , 0.53389831]), array([0.81164384, 0.47727273]), array([0.79264214, 0.504     ]), array([292, 132], dtype=int64))\n",
      "auc:\n",
      "0.644458281444583\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1166.  531.]\n",
      "[296. 128.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 123s 4s/step - loss: 0.7420 - activation_5_loss: 0.7420 - lambda_7_loss: 1.8206\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.7065 - activation_5_loss: 0.7064 - lambda_7_loss: 1.7850\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6904 - activation_5_loss: 0.6903 - lambda_7_loss: 1.7854\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6689 - activation_5_loss: 0.6689 - lambda_7_loss: 1.8034\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6463 - activation_5_loss: 0.6463 - lambda_7_loss: 1.8243\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6346 - activation_5_loss: 0.6346 - lambda_7_loss: 1.8239\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6298 - activation_5_loss: 0.6298 - lambda_7_loss: 1.8309\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6277 - activation_5_loss: 0.6276 - lambda_7_loss: 1.8300\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6192 - activation_5_loss: 0.6191 - lambda_7_loss: 1.8318\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6175 - activation_5_loss: 0.6174 - lambda_7_loss: 1.8324\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6177 - activation_5_loss: 0.6176 - lambda_7_loss: 1.8351\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6150 - activation_5_loss: 0.6150 - lambda_7_loss: 1.8289\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6140 - activation_5_loss: 0.6140 - lambda_7_loss: 1.8247\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6102 - activation_5_loss: 0.6102 - lambda_7_loss: 1.8289\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6119 - activation_5_loss: 0.6119 - lambda_7_loss: 1.8221\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6118 - activation_5_loss: 0.6118 - lambda_7_loss: 1.8214\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6112 - activation_5_loss: 0.6112 - lambda_7_loss: 1.8302\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6111 - activation_5_loss: 0.6110 - lambda_7_loss: 1.8244\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 120s 4s/step - loss: 0.6055 - activation_5_loss: 0.6055 - lambda_7_loss: 1.8226\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6061 - activation_5_loss: 0.6061 - lambda_7_loss: 1.8206\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6080 - activation_5_loss: 0.6080 - lambda_7_loss: 1.8196\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6042 - activation_5_loss: 0.6041 - lambda_7_loss: 1.8155\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6048 - activation_5_loss: 0.6048 - lambda_7_loss: 1.8138\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6007 - activation_5_loss: 0.6006 - lambda_7_loss: 1.8152\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6017 - activation_5_loss: 0.6017 - lambda_7_loss: 1.8175\n",
      "14/14 [==============================] - 10s 699ms/step\n",
      "cpu_time:\n",
      "35212.328125\n",
      "f1:\n",
      "(array([0.76807229, 0.55434783]), array([0.86148649, 0.3984375 ]), array([0.81210191, 0.46363636]), array([296, 128], dtype=int64))\n",
      "auc:\n",
      "0.6299619932432433\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1155.  542.]\n",
      "[307. 117.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 124s 4s/step - loss: 0.6709 - activation_8_loss: 0.6708 - lambda_11_loss: 2.0083\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6260 - activation_8_loss: 0.6260 - lambda_11_loss: 2.0009\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6148 - activation_8_loss: 0.6148 - lambda_11_loss: 2.0014\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6094 - activation_8_loss: 0.6094 - lambda_11_loss: 2.0010\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6043 - activation_8_loss: 0.6043 - lambda_11_loss: 1.9952\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6051 - activation_8_loss: 0.6050 - lambda_11_loss: 1.9917\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5991 - activation_8_loss: 0.5990 - lambda_11_loss: 1.9947\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5949 - activation_8_loss: 0.5948 - lambda_11_loss: 1.9915\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5944 - activation_8_loss: 0.5944 - lambda_11_loss: 1.9858\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5931 - activation_8_loss: 0.5931 - lambda_11_loss: 1.9814\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5918 - activation_8_loss: 0.5917 - lambda_11_loss: 1.9805\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5892 - activation_8_loss: 0.5891 - lambda_11_loss: 1.9836\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5941 - activation_8_loss: 0.5941 - lambda_11_loss: 1.9813\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5864 - activation_8_loss: 0.5863 - lambda_11_loss: 1.9846\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5891 - activation_8_loss: 0.5891 - lambda_11_loss: 1.9837\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5874 - activation_8_loss: 0.5874 - lambda_11_loss: 1.9842\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5847 - activation_8_loss: 0.5847 - lambda_11_loss: 1.9786\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5832 - activation_8_loss: 0.5832 - lambda_11_loss: 1.9716\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5838 - activation_8_loss: 0.5837 - lambda_11_loss: 1.9799\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5781 - activation_8_loss: 0.5781 - lambda_11_loss: 1.9715\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5792 - activation_8_loss: 0.5792 - lambda_11_loss: 1.9762\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5803 - activation_8_loss: 0.5803 - lambda_11_loss: 1.9727\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5775 - activation_8_loss: 0.5775 - lambda_11_loss: 1.9769\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5789 - activation_8_loss: 0.5789 - lambda_11_loss: 1.9725\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5784 - activation_8_loss: 0.5783 - lambda_11_loss: 1.9748\n",
      "14/14 [==============================] - 10s 706ms/step\n",
      "cpu_time:\n",
      "35190.90625\n",
      "f1:\n",
      "(array([0.81180812, 0.43137255]), array([0.71661238, 0.56410256]), array([0.76124567, 0.48888889]), array([307, 117], dtype=int64))\n",
      "auc:\n",
      "0.6403574709763635\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1175.  522.]\n",
      "[287. 137.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 124s 4s/step - loss: 0.7263 - activation_11_loss: 0.7262 - lambda_15_loss: 1.7759\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6923 - activation_11_loss: 0.6923 - lambda_15_loss: 1.7480\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6774 - activation_11_loss: 0.6774 - lambda_15_loss: 1.7477\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6595 - activation_11_loss: 0.6594 - lambda_15_loss: 1.7416\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6467 - activation_11_loss: 0.6466 - lambda_15_loss: 1.7548\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6339 - activation_11_loss: 0.6338 - lambda_15_loss: 1.7593\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6284 - activation_11_loss: 0.6284 - lambda_15_loss: 1.7641\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6225 - activation_11_loss: 0.6225 - lambda_15_loss: 1.7785\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6231 - activation_11_loss: 0.6231 - lambda_15_loss: 1.7848\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6156 - activation_11_loss: 0.6156 - lambda_15_loss: 1.7762\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6145 - activation_11_loss: 0.6145 - lambda_15_loss: 1.7947\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6136 - activation_11_loss: 0.6135 - lambda_15_loss: 1.7948\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6067 - activation_11_loss: 0.6067 - lambda_15_loss: 1.8064\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6056 - activation_11_loss: 0.6056 - lambda_15_loss: 1.8085\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6048 - activation_11_loss: 0.6047 - lambda_15_loss: 1.8111\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5998 - activation_11_loss: 0.5997 - lambda_15_loss: 1.8081\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6049 - activation_11_loss: 0.6048 - lambda_15_loss: 1.8129\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6080 - activation_11_loss: 0.6079 - lambda_15_loss: 1.8135\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5968 - activation_11_loss: 0.5967 - lambda_15_loss: 1.8138\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6037 - activation_11_loss: 0.6037 - lambda_15_loss: 1.8135\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6023 - activation_11_loss: 0.6022 - lambda_15_loss: 1.8210\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5955 - activation_11_loss: 0.5954 - lambda_15_loss: 1.8186\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5978 - activation_11_loss: 0.5977 - lambda_15_loss: 1.8251\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5929 - activation_11_loss: 0.5928 - lambda_15_loss: 1.8228\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5948 - activation_11_loss: 0.5947 - lambda_15_loss: 1.8214\n",
      "14/14 [==============================] - 11s 710ms/step\n",
      "cpu_time:\n",
      "35323.6875\n",
      "f1:\n",
      "(array([0.77018634, 0.61764706]), array([0.8641115 , 0.45985401]), array([0.81444992, 0.52719665]), array([287, 137], dtype=int64))\n",
      "auc:\n",
      "0.6619827564281899\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1178.  519.]\n",
      "[284. 140.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 124s 4s/step - loss: 0.6936 - activation_14_loss: 0.6936 - lambda_19_loss: 1.8484\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6528 - activation_14_loss: 0.6528 - lambda_19_loss: 1.8354\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6350 - activation_14_loss: 0.6350 - lambda_19_loss: 1.8325\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6337 - activation_14_loss: 0.6336 - lambda_19_loss: 1.8159\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6226 - activation_14_loss: 0.6226 - lambda_19_loss: 1.8145\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6144 - activation_14_loss: 0.6144 - lambda_19_loss: 1.8063\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6164 - activation_14_loss: 0.6164 - lambda_19_loss: 1.8053\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 122s 4s/step - loss: 0.6107 - activation_14_loss: 0.6107 - lambda_19_loss: 1.8000\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6046 - activation_14_loss: 0.6045 - lambda_19_loss: 1.7983\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6051 - activation_14_loss: 0.6051 - lambda_19_loss: 1.7959\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5989 - activation_14_loss: 0.5989 - lambda_19_loss: 1.7956\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6050 - activation_14_loss: 0.6050 - lambda_19_loss: 1.7953\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5971 - activation_14_loss: 0.5971 - lambda_19_loss: 1.7932\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6009 - activation_14_loss: 0.6008 - lambda_19_loss: 1.7869\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5940 - activation_14_loss: 0.5940 - lambda_19_loss: 1.7869\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5909 - activation_14_loss: 0.5909 - lambda_19_loss: 1.7824\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5936 - activation_14_loss: 0.5936 - lambda_19_loss: 1.7836\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5909 - activation_14_loss: 0.5908 - lambda_19_loss: 1.7830\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5864 - activation_14_loss: 0.5864 - lambda_19_loss: 1.7838\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 122s 4s/step - loss: 0.5851 - activation_14_loss: 0.5851 - lambda_19_loss: 1.7822\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5854 - activation_14_loss: 0.5853 - lambda_19_loss: 1.7850\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.5808 - activation_14_loss: 0.5808 - lambda_19_loss: 1.7776\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5833 - activation_14_loss: 0.5833 - lambda_19_loss: 1.7815\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5794 - activation_14_loss: 0.5794 - lambda_19_loss: 1.7818\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5815 - activation_14_loss: 0.5815 - lambda_19_loss: 1.7742\n",
      "14/14 [==============================] - 10s 708ms/step\n",
      "cpu_time:\n",
      "35476.046875\n",
      "f1:\n",
      "(array([0.7992278 , 0.53333333]), array([0.72887324, 0.62857143]), array([0.76243094, 0.57704918]), array([284, 140], dtype=int64))\n",
      "auc:\n",
      "0.6787223340040242\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1162.  535.]\n",
      "[300. 124.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6719 - activation_17_loss: 0.6719 - lambda_23_loss: 2.0328\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6321 - activation_17_loss: 0.6320 - lambda_23_loss: 2.0018\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6146 - activation_17_loss: 0.6146 - lambda_23_loss: 1.9953\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6051 - activation_17_loss: 0.6051 - lambda_23_loss: 1.9840\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6041 - activation_17_loss: 0.6041 - lambda_23_loss: 1.9781\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5948 - activation_17_loss: 0.5947 - lambda_23_loss: 1.9793\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5937 - activation_17_loss: 0.5936 - lambda_23_loss: 1.9809\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5898 - activation_17_loss: 0.5898 - lambda_23_loss: 1.9698\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5843 - activation_17_loss: 0.5843 - lambda_23_loss: 1.9793\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5852 - activation_17_loss: 0.5851 - lambda_23_loss: 1.9754\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5827 - activation_17_loss: 0.5827 - lambda_23_loss: 1.9628\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5804 - activation_17_loss: 0.5804 - lambda_23_loss: 1.9704\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5756 - activation_17_loss: 0.5756 - lambda_23_loss: 1.9671\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5709 - activation_17_loss: 0.5709 - lambda_23_loss: 1.9663\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5741 - activation_17_loss: 0.5741 - lambda_23_loss: 1.9647\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5731 - activation_17_loss: 0.5730 - lambda_23_loss: 1.9594\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5691 - activation_17_loss: 0.5691 - lambda_23_loss: 1.9661\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5703 - activation_17_loss: 0.5702 - lambda_23_loss: 1.9627\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5669 - activation_17_loss: 0.5668 - lambda_23_loss: 1.9666\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5660 - activation_17_loss: 0.5660 - lambda_23_loss: 1.9630\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5638 - activation_17_loss: 0.5637 - lambda_23_loss: 1.9550\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5624 - activation_17_loss: 0.5624 - lambda_23_loss: 1.9609\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5622 - activation_17_loss: 0.5622 - lambda_23_loss: 1.9524\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5605 - activation_17_loss: 0.5605 - lambda_23_loss: 1.9539\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5583 - activation_17_loss: 0.5583 - lambda_23_loss: 1.9547\n",
      "14/14 [==============================] - 11s 722ms/step\n",
      "cpu_time:\n",
      "36070.140625\n",
      "f1:\n",
      "(array([0.77643505, 0.53763441]), array([0.85666667, 0.40322581]), array([0.81458003, 0.46082949]), array([300, 124], dtype=int64))\n",
      "auc:\n",
      "0.6299462365591398\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1177.  520.]\n",
      "[285. 139.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7072 - activation_20_loss: 0.7071 - lambda_27_loss: 1.8035\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6517 - activation_20_loss: 0.6516 - lambda_27_loss: 1.8101\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6311 - activation_20_loss: 0.6310 - lambda_27_loss: 1.8053\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6203 - activation_20_loss: 0.6203 - lambda_27_loss: 1.7993\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6094 - activation_20_loss: 0.6093 - lambda_27_loss: 1.7841\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6092 - activation_20_loss: 0.6092 - lambda_27_loss: 1.7780\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6009 - activation_20_loss: 0.6008 - lambda_27_loss: 1.7787\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6029 - activation_20_loss: 0.6028 - lambda_27_loss: 1.7724\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.6015 - activation_20_loss: 0.6014 - lambda_27_loss: 1.7699\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5936 - activation_20_loss: 0.5935 - lambda_27_loss: 1.7647\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5926 - activation_20_loss: 0.5925 - lambda_27_loss: 1.7653\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5914 - activation_20_loss: 0.5914 - lambda_27_loss: 1.7655\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5952 - activation_20_loss: 0.5951 - lambda_27_loss: 1.7580\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5903 - activation_20_loss: 0.5902 - lambda_27_loss: 1.7664\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5870 - activation_20_loss: 0.5870 - lambda_27_loss: 1.7668\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5852 - activation_20_loss: 0.5852 - lambda_27_loss: 1.7651\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5862 - activation_20_loss: 0.5861 - lambda_27_loss: 1.7686\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5856 - activation_20_loss: 0.5856 - lambda_27_loss: 1.7598\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5817 - activation_20_loss: 0.5817 - lambda_27_loss: 1.7586\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5817 - activation_20_loss: 0.5817 - lambda_27_loss: 1.7593\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5855 - activation_20_loss: 0.5854 - lambda_27_loss: 1.7582\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5814 - activation_20_loss: 0.5814 - lambda_27_loss: 1.7604\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5787 - activation_20_loss: 0.5786 - lambda_27_loss: 1.7569\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5788 - activation_20_loss: 0.5787 - lambda_27_loss: 1.7607\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5782 - activation_20_loss: 0.5781 - lambda_27_loss: 1.7549\n",
      "14/14 [==============================] - 11s 714ms/step\n",
      "cpu_time:\n",
      "35958.3125\n",
      "f1:\n",
      "(array([0.78787879, 0.5984252 ]), array([0.82105263, 0.54676259]), array([0.80412371, 0.57142857]), array([285, 139], dtype=int64))\n",
      "auc:\n",
      "0.6839076107535025\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1164.  533.]\n",
      "[298. 126.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.7358 - activation_23_loss: 0.7358 - lambda_31_loss: 1.6507\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7152 - activation_23_loss: 0.7151 - lambda_31_loss: 1.6182\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7112 - activation_23_loss: 0.7112 - lambda_31_loss: 1.6125\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7071 - activation_23_loss: 0.7071 - lambda_31_loss: 1.6105\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7048 - activation_23_loss: 0.7048 - lambda_31_loss: 1.6150\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7061 - activation_23_loss: 0.7061 - lambda_31_loss: 1.6087\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7055 - activation_23_loss: 0.7055 - lambda_31_loss: 1.6070\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.7059 - activation_23_loss: 0.7058 - lambda_31_loss: 1.6129\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6973 - activation_23_loss: 0.6973 - lambda_31_loss: 1.6117\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6977 - activation_23_loss: 0.6977 - lambda_31_loss: 1.6122\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6959 - activation_23_loss: 0.6959 - lambda_31_loss: 1.6045\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6987 - activation_23_loss: 0.6986 - lambda_31_loss: 1.6080\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6966 - activation_23_loss: 0.6966 - lambda_31_loss: 1.6151\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6939 - activation_23_loss: 0.6939 - lambda_31_loss: 1.5996\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6937 - activation_23_loss: 0.6937 - lambda_31_loss: 1.6064\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6915 - activation_23_loss: 0.6915 - lambda_31_loss: 1.6029\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6817 - activation_23_loss: 0.6817 - lambda_31_loss: 1.6087\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6831 - activation_23_loss: 0.6831 - lambda_31_loss: 1.6116\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6833 - activation_23_loss: 0.6832 - lambda_31_loss: 1.6050\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6772 - activation_23_loss: 0.6771 - lambda_31_loss: 1.6022\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6763 - activation_23_loss: 0.6763 - lambda_31_loss: 1.6145\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6732 - activation_23_loss: 0.6732 - lambda_31_loss: 1.6003\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6697 - activation_23_loss: 0.6697 - lambda_31_loss: 1.6022\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6639 - activation_23_loss: 0.6639 - lambda_31_loss: 1.5966\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6655 - activation_23_loss: 0.6655 - lambda_31_loss: 1.6063\n",
      "14/14 [==============================] - 11s 721ms/step\n",
      "cpu_time:\n",
      "37146.859375\n",
      "f1:\n",
      "(array([0.77245509, 0.55555556]), array([0.86577181, 0.3968254 ]), array([0.8164557 , 0.46296296]), array([298, 126], dtype=int64))\n",
      "auc:\n",
      "0.6312986044529668\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1173.  524.]\n",
      "[289. 135.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 124s 4s/step - loss: 0.7399 - activation_26_loss: 0.7398 - lambda_35_loss: 2.1921\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6649 - activation_26_loss: 0.6649 - lambda_35_loss: 2.1488\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6401 - activation_26_loss: 0.6401 - lambda_35_loss: 2.1205\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6377 - activation_26_loss: 0.6377 - lambda_35_loss: 2.1033\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6278 - activation_26_loss: 0.6278 - lambda_35_loss: 2.0940\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6249 - activation_26_loss: 0.6249 - lambda_35_loss: 2.0850\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6190 - activation_26_loss: 0.6189 - lambda_35_loss: 2.0761\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6162 - activation_26_loss: 0.6162 - lambda_35_loss: 2.0713\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6175 - activation_26_loss: 0.6175 - lambda_35_loss: 2.0640\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6111 - activation_26_loss: 0.6110 - lambda_35_loss: 2.0639\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6134 - activation_26_loss: 0.6133 - lambda_35_loss: 2.0570\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6104 - activation_26_loss: 0.6104 - lambda_35_loss: 2.0567\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6058 - activation_26_loss: 0.6057 - lambda_35_loss: 2.0554\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6052 - activation_26_loss: 0.6051 - lambda_35_loss: 2.0466\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6060 - activation_26_loss: 0.6060 - lambda_35_loss: 2.0448\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6019 - activation_26_loss: 0.6018 - lambda_35_loss: 2.0449\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5996 - activation_26_loss: 0.5995 - lambda_35_loss: 2.0437\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5981 - activation_26_loss: 0.5981 - lambda_35_loss: 2.0438\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5990 - activation_26_loss: 0.5989 - lambda_35_loss: 2.0397\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5952 - activation_26_loss: 0.5951 - lambda_35_loss: 2.0358\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5964 - activation_26_loss: 0.5964 - lambda_35_loss: 2.0335\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5984 - activation_26_loss: 0.5983 - lambda_35_loss: 2.0326\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5924 - activation_26_loss: 0.5923 - lambda_35_loss: 2.0366\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.5904 - activation_26_loss: 0.5903 - lambda_35_loss: 2.0344\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.5943 - activation_26_loss: 0.5943 - lambda_35_loss: 2.0270\n",
      "14/14 [==============================] - 10s 707ms/step\n",
      "cpu_time:\n",
      "35716.96875\n",
      "f1:\n",
      "(array([0.76872964, 0.54700855]), array([0.816609  , 0.47407407]), array([0.79194631, 0.50793651]), array([289, 135], dtype=int64))\n",
      "auc:\n",
      "0.6453415353069333\n",
      "(2121, 192, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[1170.  527.]\n",
      "[292. 132.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "27/27 [==============================] - 124s 4s/step - loss: 0.7287 - activation_29_loss: 0.7287 - lambda_39_loss: 1.6967\n",
      "Epoch 2/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.7064 - activation_29_loss: 0.7064 - lambda_39_loss: 1.6608\n",
      "Epoch 3/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.7022 - activation_29_loss: 0.7021 - lambda_39_loss: 1.6499\n",
      "Epoch 4/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.7010 - activation_29_loss: 0.7009 - lambda_39_loss: 1.6461\n",
      "Epoch 5/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6978 - activation_29_loss: 0.6978 - lambda_39_loss: 1.6397\n",
      "Epoch 6/25\n",
      "27/27 [==============================] - 121s 4s/step - loss: 0.6996 - activation_29_loss: 0.6996 - lambda_39_loss: 1.6385\n",
      "Epoch 7/25\n",
      "27/27 [==============================] - 457s 17s/step - loss: 0.6963 - activation_29_loss: 0.6963 - lambda_39_loss: 1.6350\n",
      "Epoch 8/25\n",
      "27/27 [==============================] - 144s 5s/step - loss: 0.6958 - activation_29_loss: 0.6957 - lambda_39_loss: 1.6370\n",
      "Epoch 9/25\n",
      "27/27 [==============================] - 122s 5s/step - loss: 0.6875 - activation_29_loss: 0.6875 - lambda_39_loss: 1.6345\n",
      "Epoch 10/25\n",
      "27/27 [==============================] - 124s 5s/step - loss: 0.6922 - activation_29_loss: 0.6921 - lambda_39_loss: 1.6368\n",
      "Epoch 11/25\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.6917 - activation_29_loss: 0.6917 - lambda_39_loss: 1.6320\n",
      "Epoch 12/25\n",
      "27/27 [==============================] - 139s 5s/step - loss: 0.6946 - activation_29_loss: 0.6946 - lambda_39_loss: 1.6304\n",
      "Epoch 13/25\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.6881 - activation_29_loss: 0.6881 - lambda_39_loss: 1.6314\n",
      "Epoch 14/25\n",
      "27/27 [==============================] - 131s 5s/step - loss: 0.6930 - activation_29_loss: 0.6930 - lambda_39_loss: 1.6312\n",
      "Epoch 15/25\n",
      "27/27 [==============================] - 126s 5s/step - loss: 0.6915 - activation_29_loss: 0.6915 - lambda_39_loss: 1.6317\n",
      "Epoch 16/25\n",
      "27/27 [==============================] - 124s 5s/step - loss: 0.6902 - activation_29_loss: 0.6901 - lambda_39_loss: 1.6299\n",
      "Epoch 17/25\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.6901 - activation_29_loss: 0.6900 - lambda_39_loss: 1.6293\n",
      "Epoch 18/25\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.6909 - activation_29_loss: 0.6908 - lambda_39_loss: 1.6293\n",
      "Epoch 19/25\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.6858 - activation_29_loss: 0.6858 - lambda_39_loss: 1.6283\n",
      "Epoch 20/25\n",
      "27/27 [==============================] - 133s 5s/step - loss: 0.6892 - activation_29_loss: 0.6892 - lambda_39_loss: 1.6286\n",
      "Epoch 21/25\n",
      "27/27 [==============================] - 130s 5s/step - loss: 0.6889 - activation_29_loss: 0.6889 - lambda_39_loss: 1.6288\n",
      "Epoch 22/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6909 - activation_29_loss: 0.6908 - lambda_39_loss: 1.6269\n",
      "Epoch 23/25\n",
      "27/27 [==============================] - 133s 5s/step - loss: 0.6829 - activation_29_loss: 0.6829 - lambda_39_loss: 1.6288\n",
      "Epoch 24/25\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.6886 - activation_29_loss: 0.6885 - lambda_39_loss: 1.6283\n",
      "Epoch 25/25\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.6816 - activation_29_loss: 0.6815 - lambda_39_loss: 1.6256\n",
      "14/14 [==============================] - 12s 834ms/step\n",
      "cpu_time:\n",
      "35925.015625\n",
      "f1:\n",
      "(array([0.72451791, 0.52459016]), array([0.90068493, 0.24242424]), array([0.80305344, 0.33160622]), array([292, 132], dtype=int64))\n",
      "auc:\n",
      "0.5715545869655458\n",
      "[0.644458281444583, 0.6299619932432433, 0.6403574709763635, 0.6619827564281899, 0.6787223340040242, 0.6299462365591398, 0.6839076107535025, 0.6312986044529668, 0.6453415353069333, 0.5715545869655458]\n",
      "[0.5039999999999999, 0.4636363636363637, 0.48888888888888893, 0.5271966527196653, 0.5770491803278689, 0.4608294930875576, 0.5714285714285715, 0.46296296296296297, 0.5079365079365079, 0.3316062176165803]\n",
      "[0.5338983050847458, 0.5543478260869565, 0.43137254901960786, 0.6176470588235294, 0.5333333333333333, 0.5376344086021505, 0.5984251968503937, 0.5555555555555556, 0.5470085470085471, 0.5245901639344263]\n",
      "[0.4772727272727273, 0.3984375, 0.5641025641025641, 0.45985401459854014, 0.6285714285714286, 0.4032258064516129, 0.5467625899280576, 0.3968253968253968, 0.4740740740740741, 0.24242424242424243]\n",
      "[35058.3125, 35212.328125, 35190.90625, 35323.6875, 35476.046875, 36070.140625, 35958.3125, 37146.859375, 35716.96875, 35925.015625]\n",
      "TIME 35707.8578125 587.1936009715813\n",
      "AUC 0.6417531410134492 0.02981722426068653\n",
      "f1 0.48955348386049674 0.0659350237716948\n",
      "precision 0.5433812944299246 0.04685066734421001\n",
      "recall 0.4591550344248644 0.1027687778153484\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    start_time = process_time()\n",
    "    indices = np.arange(data.shape[0])\n",
    "    #print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    data1 = data[indices]\n",
    "    #print(data1.shape)\n",
    "    labels1 = labels[indices]\n",
    "    single_label1 = single_label[indices]\n",
    "    timeInfo1 = timeInfo[indices]\n",
    "    #print(timeInfo1.shape)\n",
    "    timeInfo1 = timeInfo1.reshape((2121, MAX_SENTS, 1))\n",
    "    print(timeInfo1.shape)\n",
    "    data1 = np.dstack((data1, timeInfo1))\n",
    "    postInfo1=postInfo[indices]\n",
    "    #print(postInfo1.shape)\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data1.shape[0])\n",
    "    zeros = np.zeros(2121)\n",
    "    zeros = zeros.reshape((2121, 1, 1))\n",
    "\n",
    "    x_train = data1[:-nb_validation_samples]\n",
    "    m=crop(1, 0, MAX_SENT_LENGTH)(x_train)\n",
    "    # print(m)\n",
    "    y_train = labels1[:-nb_validation_samples]\n",
    "    #print(y_train.shape)\n",
    "    zeros_train = zeros[:-nb_validation_samples]\n",
    "    time_train = timeInfo1[:-nb_validation_samples]\n",
    "    post_train = postInfo1[:-nb_validation_samples]\n",
    "    median_value1 = np.median(post_train)\n",
    "    post_train = np.where(np.isnan(post_train), 0, post_train)\n",
    "    x_val = data1[-nb_validation_samples:]\n",
    "    y_val = labels1[-nb_validation_samples:]\n",
    "    zeros_test = zeros[-nb_validation_samples:]\n",
    "    time_test = timeInfo1[-nb_validation_samples:]\n",
    "    post_test = postInfo1[-nb_validation_samples:]\n",
    "    median_value2 = np.median(post_test)\n",
    "    post_test = np.where(np.isnan(post_test), 0, post_test)\n",
    "    y_single = single_label1[-nb_validation_samples:]\n",
    "\n",
    "    print('Number of positive and negative posts in training and test set')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    # building Hierachical Attention network\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                POST_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True)\n",
    "\n",
    "    all_input = Input(shape=(MAX_SENT_LENGTH + 2,))\n",
    "    sentence_input = crop(1, 0, MAX_SENT_LENGTH)(all_input)  ##slice\n",
    "    time_input = crop(1, MAX_SENT_LENGTH, MAX_SENT_LENGTH + 2)(all_input)  ##slice\n",
    "    embed_dim = 200  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 200  # Hidden layer size in feed forward network inside transformer\n",
    "    vocab_size = 20000\n",
    "    embedding_layer1 = TokenAndPositionEmbedding(MAX_SENT_LENGTH, vocab_size, embed_dim)\n",
    "    x = embedding_layer1(sentence_input)\n",
    "    transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    l_trans = transformer_block1(x)\n",
    "    l_att = AttLayer(200)(l_trans)  ####(?,200)\n",
    "    # time_embedding=Dense(TIME_DIM,activation='sigmoid')(time_input)\n",
    "    merged_output = Concatenate()([l_att, time_input])  ###text+time information\n",
    "    sentEncoder = Model(all_input, merged_output)\n",
    "\n",
    "    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH+2))\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    transformer_block2 = TransformerBlock(202, num_heads, ff_dim)\n",
    "    l_lstm_sent = transformer_block2(review_encoder)\n",
    "    # pred_time=Dense(1,activation='relu')(l_lstm_sent)\n",
    "    fully_sent = Dense(1, use_bias=False)(l_lstm_sent)\n",
    "    norm_fullysent = BatchNormalization()(fully_sent)\n",
    "    pred_time = Activation(activation='linear')(norm_fullysent)\n",
    "\n",
    "    zero_input = Input(shape=(1, 1))\n",
    "    shift_predtime = Concatenate(axis=1)([zero_input, pred_time])\n",
    "    shift_predtime = crop(1, 0, MAX_SENTS)(shift_predtime)\n",
    "    l_att_sent = AttLayer(200)(l_lstm_sent)\n",
    "\n",
    "    ###embed the #likes, shares\n",
    "    post_input = Input(shape=(4,))\n",
    "    #print(post_input)\n",
    "    # post_embedding = Dense(INFO_DIM, activation='sigmoid')(post_input)\n",
    "    fully_post = Dense(INFO_DIM, use_bias=False)(post_input)\n",
    "    norm_fullypost = BatchNormalization()(fully_post)\n",
    "    post_embedding = Activation(activation='relu')(norm_fullypost)\n",
    "    x = concatenate([l_att_sent,\n",
    "                     post_embedding])  ###merge the document level vectro with the additional embedded features such as #likes\n",
    "    fully_review = Dense(2, use_bias=False)(x)\n",
    "    norm_fullyreview = BatchNormalization()(fully_review)\n",
    "    preds = Activation(activation='softmax')(norm_fullyreview)\n",
    "\n",
    "    rmsprop = optimizers.Adam(learning_rate=0.001, decay=0.99)\n",
    "    model = Model(inputs=[review_input, post_input, zero_input], outputs=[preds, shift_predtime])\n",
    "    # print(model.summary())\n",
    "    model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 0.00002],\n",
    "                  optimizer=rmsprop)\n",
    "    # filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "    # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\n",
    "\n",
    "    print(\"model fitting - Hierachical attention network for cyberbullying detection\")\n",
    "\n",
    "    model.fit([x_train, post_train, zeros_train], [y_train, time_train], batch_size=64,\n",
    "              epochs=25, verbose=1)\n",
    "    yp = model.predict([x_val, post_test, zeros_test], verbose=1)\n",
    "    ypreds = yp[0]\n",
    "    ypreds = np.argmax(ypreds, axis=1)\n",
    " #print y_single\n",
    "    #print ypred\n",
    "    f1=precision_recall_fscore_support(y_single.astype(int), ypreds)   # <==\n",
    "    auc=roc_auc_score(y_single.astype('int'), ypreds)  #<==\n",
    "    f1 = precision_recall_fscore_support(y_single.astype(int), ypreds)  # <==\n",
    "    auc = roc_auc_score(y_single.astype('int'), ypreds)  # <== category\n",
    "    end_time = process_time()\n",
    "    cpu_time = end_time - start_time\n",
    "    print(\"cpu_time:\")\n",
    "    print(cpu_time)\n",
    "    print(\"f1:\")\n",
    "    print(f1)\n",
    "    print(\"auc:\")\n",
    "    print(auc)\n",
    "    HAN_TIME.append(cpu_time)\n",
    "    HAN_AUC.append(auc)\n",
    "    HAN_f1.append(f1[2][1])\n",
    "    HAN_reca.append(f1[1][1])\n",
    "    HAN_pre.append(f1[0][1])\n",
    "\n",
    "    #for t-sne visualization\n",
    "    # if j==0:\n",
    "    #     a=model.layers\n",
    "    #     get_representations_test = K.function([model.layers[0].input,model.layers[1].input,model.layers[12].input], [model.layers[6].output])\n",
    "    #     representations_test = get_representations_test([x_val,post_test,zeros_test])[0]\n",
    "    #     representation_dict = {\n",
    "    #         'representations': representations_test,\n",
    "    #         'labels': y_single\n",
    "    #     }\n",
    "    #\n",
    "    #     with open('HANCD_Tem_results.pickle', 'wb') as handle:\n",
    "    #         pickle.dump(representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # K.clear_session()\n",
    "\n",
    "print(HAN_AUC)\n",
    "print(HAN_f1)\n",
    "print(HAN_pre)\n",
    "print(HAN_reca)\n",
    "print(HAN_TIME)\n",
    "print (\"TIME\",np.mean(HAN_TIME), np.std(HAN_TIME))\n",
    "print (\"AUC\",np.mean(HAN_AUC), np.std(HAN_AUC))\n",
    "print (\"f1\", np.mean(HAN_f1), np.std(HAN_f1))\n",
    "print (\"precision\",np.mean(HAN_pre), np.std(HAN_pre))\n",
    "print (\"recall\", np.mean(HAN_reca), np.std(HAN_reca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2beea49-b28e-4960-a7ad-a1a516ed314a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:56:50.337603Z",
     "iopub.status.busy": "2023-05-28T03:56:50.336881Z",
     "iopub.status.idle": "2023-05-28T03:56:50.414668Z",
     "shell.execute_reply": "2023-05-28T03:56:50.412113Z",
     "shell.execute_reply.started": "2023-05-28T03:56:50.337541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 192, 22)]    0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, 192, 202)    4286400     ['input_24[0][0]']               \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 30)           30          ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " transformer_block_13 (Transfor  (None, 192, 202)    246034      ['time_distributed_6[0][0]']     \n",
      " merBlock)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 30)          120         ['dense_100[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " att_layer_12 (AttLayer)        (None, 202)          40800       ['transformer_block_13[0][0]']   \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 30)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_99 (Dense)               (None, 192, 1)       202         ['transformer_block_13[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 232)          0           ['att_layer_12[0][0]',           \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 192, 1)      4           ['dense_99[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 2)            464         ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 1, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 192, 1)       0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 2)           8           ['dense_101[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 193, 1)       0           ['input_25[0][0]',               \n",
      "                                                                  'activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 2)            0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_26 (Lambda)             (None, 192, 1)       0           ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,574,062\n",
      "Trainable params: 4,573,996\n",
      "Non-trainable params: 66\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9755a78a-1238-4016-8911-e2ad435fdfec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-28T03:57:03.290318Z",
     "iopub.status.busy": "2023-05-28T03:57:03.289589Z",
     "iopub.status.idle": "2023-05-28T03:57:03.329722Z",
     "shell.execute_reply": "2023-05-28T03:57:03.327982Z",
     "shell.execute_reply.started": "2023-05-28T03:57:03.290255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_24 (Lambda)             (None, 20)           0           ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_6  (None, 20, 200)     4004000     ['lambda_24[0][0]']              \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " transformer_block_12 (Transfor  (None, 20, 200)     242000      ['token_and_position_embedding_6[\n",
      " merBlock)                                                       0][0]']                          \n",
      "                                                                                                  \n",
      " att_layer_11 (AttLayer)        (None, 200)          40400       ['transformer_block_12[0][0]']   \n",
      "                                                                                                  \n",
      " lambda_25 (Lambda)             (None, 2)            0           ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 202)          0           ['att_layer_11[0][0]',           \n",
      "                                                                  'lambda_25[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,286,400\n",
      "Trainable params: 4,286,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a567bae-5b16-49c3-b75f-c09071cbffee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
