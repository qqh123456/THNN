{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4769e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid</th>\n",
       "      <th>text</th>\n",
       "      <th>comment</th>\n",
       "      <th>time</th>\n",
       "      <th>poster</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MFl4csvfW</td>\n",
       "      <td>åŸæ¥å¤§å®¶æ‰€ææƒ§çš„16å·è£åˆ¤æ­£æ˜¯ä»¤ç§‘å°”å£åå£é¦™ç³–çš„é‚£ä½î˜§é‡‘å·å‹‡å£«#æ¹–äººå¯¹æˆ˜å‹‡å£«#LLOVE_ç»´...</td>\n",
       "      <td>['å°±æ˜¯ä»–èµŒçƒ', 'äººå®¶å¯æ˜¯æ­ªå˜´æˆ˜ç¥', 'å˜´æ°”æ­ªçš„é‚£ä½', 'æ‰€ä»¥å‘¢ä¸»åœºç»™å®‰æ’å‡¯å°”ç‰¹äººä¸...</td>\n",
       "      <td>[05æœˆ06æ—¥ 22:39, 05æœˆ06æ—¥ 22:26, 05æœˆ06æ—¥ 22:23, 05æœˆ...</td>\n",
       "      <td>167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF8iLkA7H</td>\n",
       "      <td>#æ¹–äººvså‹‡å£«#@Hå’Œ30:ã€Šå·ä¸€ä¸ªä¸»åœºå°±å¤Ÿäº†ã€‹ã€Šé‡‘å·è£åˆ¤é˜Ÿã€‹ã€Šå…‹è±ä¸å¯èƒ½åœºåœºè¿™æ ·ã€‹ã€Šä»Šå¤©æ¹–...</td>\n",
       "      <td>['å‹‡èœœåˆå¦‚æ„äº†æ˜¯å§', 'å»å¹´å¤ªé˜³è¾“ç‹¬è¡Œä¾ ç¬¬ä¸‰åœºæ—¶ä¹Ÿæ˜¯é“ºå¤©ç›–åœ°çš„å¤ªé˜³æ”¾æ°´è®º[é€èŠ±èŠ±]', ...</td>\n",
       "      <td>[05æœˆ05æ—¥ 16:22, 05æœˆ05æ—¥ 17:15, 05æœˆ05æ—¥ 15:58, 05æœˆ...</td>\n",
       "      <td>79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MF8Qjy6UC</td>\n",
       "      <td>æˆ‘å‘äº†ä¸€æ¡è¿™ä¸ªæ¯”èµ›å‹‡å£«è¾“ä¸äº†ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆå¾ˆå¤šäººç†è§£æˆæˆ‘è¯´çš„æ˜¯è£åˆ¤ã€‚è£åˆ¤å°ºåº¦è‚¯å®šæ˜¯ä¼šå½±å“æ¯”èµ›...</td>\n",
       "      <td>['gswin5', 'æ˜¯çš„ï¼Œå¼€åœºå°±æ‡’æ‡’æ•£æ•£çš„ï¼ŒåŠ ä¸Šè£åˆ¤è¿™ä¹ˆç…§é¡¾æµ“çœ‰ï¼Œé‚£å°±æ²¡å¾—æ‰“äº†', 'è£...</td>\n",
       "      <td>[05æœˆ05æ—¥ 20:38, 05æœˆ05æ—¥ 20:28, 05æœˆ05æ—¥ 18:55, 05æœˆ...</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF7eQ72mG</td>\n",
       "      <td>èŠ±äº†æ˜¯å‹‡å£«ä¸‰åˆ†æŠ•çš„å‡†ï¼Œæ¹–äººè‡ªå·±é˜²å®ˆå·®ï¼Œæ‰“å¾—ä¸å¥½ï¼Œè·Ÿè£åˆ¤ä¹Ÿæ²¡å…³ç³»ã€‚æˆ‘ä¹Ÿä¸è®¤å¯å‡çƒçš„è¯´æ³•ï¼Œæˆ‘æ²¡è§‰...</td>\n",
       "      <td>['æ¹–äººçƒè¿·æ€¥äº†', 'ä½†æ˜¯æ¹–äººæ²¡ä¸Šå¼ºåº¦ä¹Ÿæ˜¯çœŸçš„ï¼Œæ„Ÿè§‰å°±æ˜¯åœ¨æŠ•ç¯®è®­ç»ƒï¼Œäº’ç›¸æŠ•ç¯®å°±è¢«å‹‡å£«æŠ•èŠ±äº†...</td>\n",
       "      <td>[05æœˆ05æ—¥ 11:06, 05æœˆ05æ—¥ 11:06, 05æœˆ05æ—¥ 11:07, 05æœˆ...</td>\n",
       "      <td>429</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF7dZ7U1S</td>\n",
       "      <td>è¿™è¦æ˜¯æ ¼æ—å’Œæ–½ç½—å¾·è§’è‰²äº’æ¢ä¸€ä¸‹...çƒè¿·èƒ½æ‰¹æ–—è°©éª‚æ•´ä¸ªå­£åèµ›è£åˆ¤:ç”±äºæ ¼æ—æœ‰ä»€ä¹ˆä»€ä¹ˆå†å²ï¼Œæ‰€...</td>\n",
       "      <td>['çœŸçš„å®¹æ˜“æ‹‰ä¼¤å†…ä¾§è‚Œè‚‰', 'æ–½ç½—å¾·å°åŠ¨ä½œå¾ˆå¤šçš„', 'è©¹å¯†çœ‹ä¸è§ç³»åˆ—', 'è£¤èœœçœ‹å¾—è§ï¼Ÿ...</td>\n",
       "      <td>[05æœˆ05æ—¥ 11:08, 05æœˆ05æ—¥ 11:43, 05æœˆ05æ—¥ 19:42, 05æœˆ...</td>\n",
       "      <td>378</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Mp3Pmr9CB</td>\n",
       "      <td>2022å¹´ï¼Œæˆ‘ä»¬åœ¨è®©äººçª’æ¯çš„ç–«æƒ…é—´éš™é‡Œä¸ºäº†ä¸€åœºåˆä¸€åœºå¹¸ç¦å¥”èµ°ã€‚å¹¿ä¸œã€ç¦å»ºã€æµ™æ±Ÿã€æ±Ÿè‹ã€å®‰å¾½ã€...</td>\n",
       "      <td>['', 'æ€ä¹ˆåšåˆ°çš„ï¼Œå¤ªç‰›äº†ï¼', 'ç¥–å›½å¤§å¥½æ²³å±±']</td>\n",
       "      <td>[01æœˆ20æ—¥ 09:57, 01æœˆ20æ—¥ 09:50, 01æœˆ19æ—¥ 22:01]</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>MoV7e2LHs</td>\n",
       "      <td>î˜§èƒ¡æ˜è½©å°èƒ¡ç­¾åç­¾åˆ°å¿˜æˆ‘ï¼Œä¸Šäº†è¾½å®é˜Ÿçš„è½¦ï¼Œå°èƒ¡ï¼šèµ¶ç´§æºœï¼Œä½ ä»¬å•¥ä¹Ÿæ²¡çœ‹è§#èƒ¡æ˜è½©##å¹¿ä¸œå®è¿œ#...</td>\n",
       "      <td>['å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆä½ çš„ç¬”æ‹¿å›æ¥äº†å—', 'ğŸ¯ï¼šå¿«è·‘ä¸¢æ­»ä¸ªäºº', 'å¯¹ä¸èµ·å“¥ä½†æ˜¯è¿˜æ˜¯æƒ³ç¬‘å“ˆå“ˆå“ˆ...</td>\n",
       "      <td>[01æœˆ18æ—¥ 22:52, 01æœˆ18æ—¥ 22:56, 01æœˆ18æ—¥ 22:59, 01æœˆ...</td>\n",
       "      <td>557</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>MoZqCfoA5</td>\n",
       "      <td>è¾½ç²¤å¤§æˆ˜ï½œéƒ­è‰¾ä¼¦14åˆ†6ç¯®æ¿6åŠ©æ”»ï¼Œå¼ é•‡éºŸ10åˆ†4ç¯®æ¿3åŠ©æ”»2æ–­ï½œå¹¿ä¸œVSè¾½å®ï½œCBAå¸¸è§„èµ›...</td>\n",
       "      <td>['æ˜¯çš„ï¼Œæœå³°å¤ªäº†è§£äº†ä»–äº†ï¼Œåªè¦éƒ­è‰¾ä¼¦æŒçƒå°±çŸ¥é“ä»–ä¸‹ä¸€ä¸ªåŠ¨ä½œäº†ï¼Œæ‰€ä»¥çœŸçš„è·Ÿå›°éš¾ï¼Œè™½ç„¶è¯´ä¸èƒ½æ‰¾...</td>\n",
       "      <td>[01æœˆ19æ—¥ 13:56, 01æœˆ19æ—¥ 11:43, 01æœˆ19æ—¥ 11:05, 01æœˆ...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>MpvTn2kBR</td>\n",
       "      <td>ä»Šå¹´åªæœ‰å¤®è§†æ˜¥æ™šçš„è§‚ä¼—æ²¡æœ‰æˆ´å£ç½©ï¼Œå…¶å®ƒåœ°æ–¹å°ï¼ˆåŒ…æ‹¬åŒ—äº¬å«è§†ï¼Œä¸Šæµ·å«è§†ï¼Œè¾½å®å«è§†ï¼Œä¸œæ–¹å«è§†ï¼Œå¹¿...</td>\n",
       "      <td>['åœ°æ–¹æå‰å½•çš„ï¼Œé‚£æ—¶å€™ä¸æ¸…æ¥šå¤®è§†æ€åº¦ï¼Œæ±‚ç¨³å°±æˆ´ç€äº†', 'åœ°æ–¹å°æå‰æ›´æ—©å½•å¥½çš„å§', 'ä¸...</td>\n",
       "      <td>[01æœˆ22æ—¥ 21:27, 01æœˆ22æ—¥ 20:28, 01æœˆ23æ—¥ 08:40, 01æœˆ...</td>\n",
       "      <td>268</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>MpUPCF0Vr</td>\n",
       "      <td>#æ˜¥èŠ‚æ—…æ¸¸#ã€å…”å¹´æ˜¥èŠ‚è¿‡åŠï¼Œå¤ªåŸé—¨ç¥¨é¢„è®¢åŒæ¯”æ¶¨952%ã€‘1æœˆ25æ—¥ï¼Œå…”å¹´æ˜¥èŠ‚å‡æœŸå·²ç»è¿‡åŠã€‚æº...</td>\n",
       "      <td>['å¤§å®¶éƒ½ä¸æ€•ç–«æƒ…äº†', 'æ˜¥èŠ‚å¤§å®¶éƒ½å»æ—…æ¸¸äº†', 'å¤§å®¶éƒ½æƒ³å‡ºå»èµ°èµ°äº†', 'æœ‰ç©ºå»', ...</td>\n",
       "      <td>[01æœˆ25æ—¥ 12:33, 01æœˆ25æ—¥ 12:08, 01æœˆ25æ—¥ 12:00, 01æœˆ...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bid                                               text  \\\n",
       "0    MFl4csvfW  åŸæ¥å¤§å®¶æ‰€ææƒ§çš„16å·è£åˆ¤æ­£æ˜¯ä»¤ç§‘å°”å£åå£é¦™ç³–çš„é‚£ä½î˜§é‡‘å·å‹‡å£«#æ¹–äººå¯¹æˆ˜å‹‡å£«#LLOVE_ç»´...   \n",
       "1    MF8iLkA7H  #æ¹–äººvså‹‡å£«#@Hå’Œ30:ã€Šå·ä¸€ä¸ªä¸»åœºå°±å¤Ÿäº†ã€‹ã€Šé‡‘å·è£åˆ¤é˜Ÿã€‹ã€Šå…‹è±ä¸å¯èƒ½åœºåœºè¿™æ ·ã€‹ã€Šä»Šå¤©æ¹–...   \n",
       "2    MF8Qjy6UC  æˆ‘å‘äº†ä¸€æ¡è¿™ä¸ªæ¯”èµ›å‹‡å£«è¾“ä¸äº†ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆå¾ˆå¤šäººç†è§£æˆæˆ‘è¯´çš„æ˜¯è£åˆ¤ã€‚è£åˆ¤å°ºåº¦è‚¯å®šæ˜¯ä¼šå½±å“æ¯”èµ›...   \n",
       "3    MF7eQ72mG  èŠ±äº†æ˜¯å‹‡å£«ä¸‰åˆ†æŠ•çš„å‡†ï¼Œæ¹–äººè‡ªå·±é˜²å®ˆå·®ï¼Œæ‰“å¾—ä¸å¥½ï¼Œè·Ÿè£åˆ¤ä¹Ÿæ²¡å…³ç³»ã€‚æˆ‘ä¹Ÿä¸è®¤å¯å‡çƒçš„è¯´æ³•ï¼Œæˆ‘æ²¡è§‰...   \n",
       "4    MF7dZ7U1S  è¿™è¦æ˜¯æ ¼æ—å’Œæ–½ç½—å¾·è§’è‰²äº’æ¢ä¸€ä¸‹...çƒè¿·èƒ½æ‰¹æ–—è°©éª‚æ•´ä¸ªå­£åèµ›è£åˆ¤:ç”±äºæ ¼æ—æœ‰ä»€ä¹ˆä»€ä¹ˆå†å²ï¼Œæ‰€...   \n",
       "..         ...                                                ...   \n",
       "820  Mp3Pmr9CB  2022å¹´ï¼Œæˆ‘ä»¬åœ¨è®©äººçª’æ¯çš„ç–«æƒ…é—´éš™é‡Œä¸ºäº†ä¸€åœºåˆä¸€åœºå¹¸ç¦å¥”èµ°ã€‚å¹¿ä¸œã€ç¦å»ºã€æµ™æ±Ÿã€æ±Ÿè‹ã€å®‰å¾½ã€...   \n",
       "821  MoV7e2LHs  î˜§èƒ¡æ˜è½©å°èƒ¡ç­¾åç­¾åˆ°å¿˜æˆ‘ï¼Œä¸Šäº†è¾½å®é˜Ÿçš„è½¦ï¼Œå°èƒ¡ï¼šèµ¶ç´§æºœï¼Œä½ ä»¬å•¥ä¹Ÿæ²¡çœ‹è§#èƒ¡æ˜è½©##å¹¿ä¸œå®è¿œ#...   \n",
       "822  MoZqCfoA5  è¾½ç²¤å¤§æˆ˜ï½œéƒ­è‰¾ä¼¦14åˆ†6ç¯®æ¿6åŠ©æ”»ï¼Œå¼ é•‡éºŸ10åˆ†4ç¯®æ¿3åŠ©æ”»2æ–­ï½œå¹¿ä¸œVSè¾½å®ï½œCBAå¸¸è§„èµ›...   \n",
       "823  MpvTn2kBR  ä»Šå¹´åªæœ‰å¤®è§†æ˜¥æ™šçš„è§‚ä¼—æ²¡æœ‰æˆ´å£ç½©ï¼Œå…¶å®ƒåœ°æ–¹å°ï¼ˆåŒ…æ‹¬åŒ—äº¬å«è§†ï¼Œä¸Šæµ·å«è§†ï¼Œè¾½å®å«è§†ï¼Œä¸œæ–¹å«è§†ï¼Œå¹¿...   \n",
       "824  MpUPCF0Vr  #æ˜¥èŠ‚æ—…æ¸¸#ã€å…”å¹´æ˜¥èŠ‚è¿‡åŠï¼Œå¤ªåŸé—¨ç¥¨é¢„è®¢åŒæ¯”æ¶¨952%ã€‘1æœˆ25æ—¥ï¼Œå…”å¹´æ˜¥èŠ‚å‡æœŸå·²ç»è¿‡åŠã€‚æº...   \n",
       "\n",
       "                                               comment  \\\n",
       "0    ['å°±æ˜¯ä»–èµŒçƒ', 'äººå®¶å¯æ˜¯æ­ªå˜´æˆ˜ç¥', 'å˜´æ°”æ­ªçš„é‚£ä½', 'æ‰€ä»¥å‘¢ä¸»åœºç»™å®‰æ’å‡¯å°”ç‰¹äººä¸...   \n",
       "1    ['å‹‡èœœåˆå¦‚æ„äº†æ˜¯å§', 'å»å¹´å¤ªé˜³è¾“ç‹¬è¡Œä¾ ç¬¬ä¸‰åœºæ—¶ä¹Ÿæ˜¯é“ºå¤©ç›–åœ°çš„å¤ªé˜³æ”¾æ°´è®º[é€èŠ±èŠ±]', ...   \n",
       "2    ['gswin5', 'æ˜¯çš„ï¼Œå¼€åœºå°±æ‡’æ‡’æ•£æ•£çš„ï¼ŒåŠ ä¸Šè£åˆ¤è¿™ä¹ˆç…§é¡¾æµ“çœ‰ï¼Œé‚£å°±æ²¡å¾—æ‰“äº†', 'è£...   \n",
       "3    ['æ¹–äººçƒè¿·æ€¥äº†', 'ä½†æ˜¯æ¹–äººæ²¡ä¸Šå¼ºåº¦ä¹Ÿæ˜¯çœŸçš„ï¼Œæ„Ÿè§‰å°±æ˜¯åœ¨æŠ•ç¯®è®­ç»ƒï¼Œäº’ç›¸æŠ•ç¯®å°±è¢«å‹‡å£«æŠ•èŠ±äº†...   \n",
       "4    ['çœŸçš„å®¹æ˜“æ‹‰ä¼¤å†…ä¾§è‚Œè‚‰', 'æ–½ç½—å¾·å°åŠ¨ä½œå¾ˆå¤šçš„', 'è©¹å¯†çœ‹ä¸è§ç³»åˆ—', 'è£¤èœœçœ‹å¾—è§ï¼Ÿ...   \n",
       "..                                                 ...   \n",
       "820                       ['', 'æ€ä¹ˆåšåˆ°çš„ï¼Œå¤ªç‰›äº†ï¼', 'ç¥–å›½å¤§å¥½æ²³å±±']   \n",
       "821  ['å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆå“ˆä½ çš„ç¬”æ‹¿å›æ¥äº†å—', 'ğŸ¯ï¼šå¿«è·‘ä¸¢æ­»ä¸ªäºº', 'å¯¹ä¸èµ·å“¥ä½†æ˜¯è¿˜æ˜¯æƒ³ç¬‘å“ˆå“ˆå“ˆ...   \n",
       "822  ['æ˜¯çš„ï¼Œæœå³°å¤ªäº†è§£äº†ä»–äº†ï¼Œåªè¦éƒ­è‰¾ä¼¦æŒçƒå°±çŸ¥é“ä»–ä¸‹ä¸€ä¸ªåŠ¨ä½œäº†ï¼Œæ‰€ä»¥çœŸçš„è·Ÿå›°éš¾ï¼Œè™½ç„¶è¯´ä¸èƒ½æ‰¾...   \n",
       "823  ['åœ°æ–¹æå‰å½•çš„ï¼Œé‚£æ—¶å€™ä¸æ¸…æ¥šå¤®è§†æ€åº¦ï¼Œæ±‚ç¨³å°±æˆ´ç€äº†', 'åœ°æ–¹å°æå‰æ›´æ—©å½•å¥½çš„å§', 'ä¸...   \n",
       "824  ['å¤§å®¶éƒ½ä¸æ€•ç–«æƒ…äº†', 'æ˜¥èŠ‚å¤§å®¶éƒ½å»æ—…æ¸¸äº†', 'å¤§å®¶éƒ½æƒ³å‡ºå»èµ°èµ°äº†', 'æœ‰ç©ºå»', ...   \n",
       "\n",
       "                                                  time  poster  lable  \n",
       "0    [05æœˆ06æ—¥ 22:39, 05æœˆ06æ—¥ 22:26, 05æœˆ06æ—¥ 22:23, 05æœˆ...     167    0.0  \n",
       "1    [05æœˆ05æ—¥ 16:22, 05æœˆ05æ—¥ 17:15, 05æœˆ05æ—¥ 15:58, 05æœˆ...      79    0.0  \n",
       "2    [05æœˆ05æ—¥ 20:38, 05æœˆ05æ—¥ 20:28, 05æœˆ05æ—¥ 18:55, 05æœˆ...      69    0.0  \n",
       "3    [05æœˆ05æ—¥ 11:06, 05æœˆ05æ—¥ 11:06, 05æœˆ05æ—¥ 11:07, 05æœˆ...     429    1.0  \n",
       "4    [05æœˆ05æ—¥ 11:08, 05æœˆ05æ—¥ 11:43, 05æœˆ05æ—¥ 19:42, 05æœˆ...     378    1.0  \n",
       "..                                                 ...     ...    ...  \n",
       "820         [01æœˆ20æ—¥ 09:57, 01æœˆ20æ—¥ 09:50, 01æœˆ19æ—¥ 22:01]      53    0.0  \n",
       "821  [01æœˆ18æ—¥ 22:52, 01æœˆ18æ—¥ 22:56, 01æœˆ18æ—¥ 22:59, 01æœˆ...     557    0.0  \n",
       "822  [01æœˆ19æ—¥ 13:56, 01æœˆ19æ—¥ 11:43, 01æœˆ19æ—¥ 11:05, 01æœˆ...      55    0.0  \n",
       "823  [01æœˆ22æ—¥ 21:27, 01æœˆ22æ—¥ 20:28, 01æœˆ23æ—¥ 08:40, 01æœˆ...     268    0.0  \n",
       "824  [01æœˆ25æ—¥ 12:33, 01æœˆ25æ—¥ 12:08, 01æœˆ25æ—¥ 12:00, 01æœˆ...      38    0.0  \n",
       "\n",
       "[825 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Concatenate, BatchNormalization, Activation, Flatten,GlobalAveragePooling1D\n",
    "from keras.layers import Lambda, Embedding, GRU, Bidirectional, TimeDistributed, concatenate\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from word2vecReader import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "# from tensorflow.python.keras.optimizers import adam_v2\n",
    "# from tensorflow.python.keras.optimizers import rmsprop_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from  keras import layers\n",
    "from time import process_time\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "np.random.seed(0)\n",
    "MAX_SENT_LENGTH = 20  #number of words in a sentence\n",
    "MAX_NB_WORDS = 20000\n",
    "POST_DIM = 123\n",
    "INFO_DIM = 30\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "##slice tensor function in keras\n",
    "def crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "\n",
    "    return Lambda(func)\n",
    "\n",
    "\n",
    "def myFunc(x):\n",
    "    if \"empety\" in x:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = string.strip().lower()\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_words = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def find_str(s, char):\n",
    "    index = 0\n",
    "\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index + len(char)] == char:\n",
    "                    return index\n",
    "\n",
    "            index += 1\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weightss = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    \n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''Transformerçš„Encoderéƒ¨åˆ†'''\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "'''Transformerè¾“å…¥çš„ç¼–ç å±‚'''\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# æ–‡ä»¶å¤¹è·¯å¾„\n",
    "folder_path = 'C:/Users/Administrator/Desktop/keyan dateset/å¾®åšæ­£æ–‡'\n",
    "\n",
    "# åˆ›å»ºåŒ…å«ä¸‰ä¸ªå…³é”®å­—çš„å­—å…¸\n",
    "data_dict = {\n",
    "    'bid': [],\n",
    "    'text': [],\n",
    "    'comment': [],\n",
    "    'time': [],\n",
    "    'poster': [],\n",
    "    'lable':[]\n",
    "}\n",
    "\n",
    "# éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # æ‰“å¼€ Excel æ–‡ä»¶\n",
    "        workbook = openpyxl.load_workbook(file_path)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        # æå–æ•°æ®\n",
    "        for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "            bid = row[1]\n",
    "            text = row[5]\n",
    "            likes = row[8]\n",
    "\n",
    "            # å°†æ•°æ®æ·»åŠ åˆ°å­—å…¸å¯¹åº”çš„åˆ—è¡¨ä¸­\n",
    "            data_dict['bid'].append(bid)\n",
    "            data_dict['text'].append(text)\n",
    "            data_dict['poster'].append(likes)\n",
    "\n",
    "        # å…³é—­ Excel æ–‡ä»¶\n",
    "        workbook.close()\n",
    "\n",
    "bid_length = len(data_dict['bid'])\n",
    "for i,bid in enumerate(data_dict['bid']):\n",
    "    # æ„å»ºæ–‡ä»¶è·¯å¾„\n",
    "    for root, directories, files in os.walk('C:/Users/Administrator/Desktop/keyan dateset/comment'):\n",
    "        for file in files:\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            if(str(bid)==str(file_name)):\n",
    "                file_path = os.path.join(root, file)\n",
    "                #print(file_path)\n",
    "                workbook = xlrd.open_workbook(file_path)\n",
    "                sheet = workbook.sheet_by_index(0)  # å‡è®¾è¦è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨\n",
    "\n",
    "                # éå†å·¥ä½œè¡¨çš„è¡Œ\n",
    "                comment=[]\n",
    "                time=[]\n",
    "                lable=sheet.cell_value(1, 5)\n",
    "                cell = sheet.cell(1, 5)\n",
    "                lable = cell.value\n",
    "\n",
    "                # å¦‚æœå•å…ƒæ ¼ä¸ºç©ºï¼Œå°†å…¶è§†ä¸º0\n",
    "                if cell.ctype == xlrd.XL_CELL_EMPTY:\n",
    "                    lable = 0\n",
    "\n",
    "                for row in range(1, sheet.nrows):\n",
    "                    comment1 = sheet.cell_value(row,2)\n",
    "                    time1 = sheet.cell_value(row,3)\n",
    "                    comment.append(comment1)\n",
    "                    time.append(time1)\n",
    "                data_dict['lable'].append(lable)\n",
    "                data_dict['comment'].append(comment)\n",
    "                data_dict['time'].append(time)\n",
    "        # å…³é—­ Excel æ–‡ä»¶\n",
    "                workbook.release_resources()\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# åˆ é™¤commentåˆ—ä¸ºç©ºåˆ—è¡¨çš„è¡Œ\n",
    "df = df[df['comment'].apply(len) > 0]\n",
    "df = df[df['time'].apply(lambda x: len(x[0]) != 6)]\n",
    "# åˆ é™¤commentå’Œtimeå…·æœ‰ç›¸åŒå€¼çš„è¡Œ\n",
    "df = df[~((df['comment'] == df['time']) & (df['comment'].apply(len) > 0))]\n",
    "df['comment'] = df['comment'].astype(str)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\<.*?\\>', '', x))  # å»é™¤æ–¹æ‹¬å·åŠå…¶å†…å®¹\n",
    "df['comment'] = df['comment'].apply(lambda x: re.sub(r'\\<.*?\\>', '', x))  # å»é™¤æ–¹æ‹¬å·åŠå…¶å†…å®¹\n",
    "# é‡ç½®ç´¢å¼•\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# æ‰“å° DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b5135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time=df['time']\n",
    "\n",
    "time1_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe78300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_list in time:\n",
    "    if len(time_list[0]) > 6:\n",
    "        base_time = datetime.strptime(time_list[0], '%mæœˆ%dæ—¥ %H:%M')\n",
    "    else:\n",
    "        base_time = datetime.strptime(time_list[0] + ' 00:00', '%mæœˆ%dæ—¥ %H:%M')\n",
    "    time_diff_list = []\n",
    "    for time_str in time_list:\n",
    "        if len(time_str) > 8:\n",
    "            current_time = datetime.strptime(time_str, '%mæœˆ%dæ—¥ %H:%M')\n",
    "        else:\n",
    "            current_time = datetime.strptime(time_str + ' 00:00', '%mæœˆ%dæ—¥ %H:%M')\n",
    "        time_diff = (current_time - base_time).total_seconds()\n",
    "        time_diff_list.append(time_diff)\n",
    "    time1_list.append(time_diff_list)\n",
    "#print(time1_list)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992f014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 7189 unique tokens.\n",
      "Shape of data tensor: (825, 490, 21)\n",
      "Shape of label tensor: (825, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: overflow encountered in square\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: invalid value encountered in square\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "E:\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: invalid value encountered in divide\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "texts = df['text']\n",
    "texts = texts.fillna(\"\")\n",
    "# texts=[text.encode('ascii') for text in texts]\n",
    "#print(texts)\n",
    "comments = df['comment']\n",
    "#print(comments)\n",
    "timeInfo = time1_list\n",
    "#print(timeInfo)\n",
    "postInfo = df['poster']\n",
    "#print(postInfo)\n",
    "\n",
    "labels = df['lable']\n",
    "#print(labels)\n",
    "b = np.zeros([len(timeInfo), len(max(timeInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(timeInfo):\n",
    "    b[i][0:len(j)] = j\n",
    "timeInfo = b\n",
    "time_size = len(np.unique(timeInfo))\n",
    "MAX_SENTS = len(timeInfo[0])  ####number of sentences\n",
    "\n",
    "postInfo = np.array(postInfo)\n",
    "#print(postInfo)\n",
    "post_size = len(np.unique(postInfo))\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "#tokenizer.fit_on_texts(texts)\n",
    "#print(MAX_SENTS)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH+1), dtype='int32')\n",
    "#print(tokenizer.word_index['club'])\n",
    "\n",
    "texts = texts.apply(lambda x: list(jieba.cut(x)))\n",
    "word_count = Counter()\n",
    "for text_seg in texts:\n",
    "    word_count.update(text_seg)\n",
    "word_list = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = {}\n",
    "for i, (word, freq) in enumerate(word_list):\n",
    "    vocab[word] = i\n",
    "\n",
    "#comments = comments.apply(lambda x: list(jieba.cut(x)))\n",
    "#comments = comments.apply(lambda x: [vocab[word] for word in x if word in vocab])\n",
    "m = 0\n",
    "n = 0\n",
    "for m, sentences in enumerate(comments):\n",
    "    sentences = sentences.split(',')\n",
    "    for n, sent in enumerate(sentences):\n",
    "        if n < MAX_SENTS:\n",
    "            k = 0\n",
    "            sent = list(jieba.cut(sent))\n",
    "            for word in sent:\n",
    "                if word in vocab:\n",
    "                    data[m, n, k] = vocab[word]\n",
    "                    k = k + 1\n",
    "                if k >= MAX_SENT_LENGTH:\n",
    "                    break\n",
    "\n",
    "word_index = vocab\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "single_label = np.asarray(labels)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "###standardize\n",
    "timeInfo = preprocessing.StandardScaler().fit_transform(timeInfo)\n",
    "postInfo = preprocessing.StandardScaler().fit_transform(postInfo.reshape(-1, 1))\n",
    "HAN_pre = []\n",
    "HAN_reca = []\n",
    "HAN_f1 = []\n",
    "HAN_AUC = []\n",
    "HAN_TIME = []\n",
    "embeddings_index = Word2Vec.load_word2vec_format(\"word2vec_model.bin\", binary=True, )  #\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, POST_DIM))\n",
    "outword_dic = dict()\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        new_vector = np.random.rand(POST_DIM, )\n",
    "        outword_dic.setdefault(word, new_vector)\n",
    "        embedding_matrix[i] = outword_dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f690e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825, 490, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[391. 269.]\n",
      "[107.  58.]\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "10/11 [==========================>...] - ETA: 10s - loss: 0.6800 - activation_2_loss: 0.6800 - lambda_3_loss: 1.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 104\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel fitting - Hierachical attention network for cyberbullying detection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeros_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m yp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([x_val, post_test, zeros_test], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    107\u001b[0m ypreds \u001b[38;5;241m=\u001b[39m yp[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    start_time = process_time()\n",
    "    indices = np.arange(data.shape[0])\n",
    "    #print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    data1 = data[indices]\n",
    "    #print(data1.shape)\n",
    "    labels1 = labels[indices]\n",
    "    single_label1 = single_label[indices]\n",
    "    timeInfo1 = timeInfo[indices]\n",
    "    #print(timeInfo1.shape)\n",
    "    timeInfo1 = timeInfo1.reshape((825, MAX_SENTS, 1))\n",
    "    print(timeInfo1.shape)\n",
    "    data1 = np.dstack((data1, timeInfo1))\n",
    "    postInfo1=postInfo[indices]\n",
    "    #print(postInfo1.shape)\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data1.shape[0])\n",
    "    zeros = np.zeros(825)\n",
    "    zeros = zeros.reshape((825, 1, 1))\n",
    "\n",
    "    x_train = data1[:-nb_validation_samples]\n",
    "    m=crop(1, 0, MAX_SENT_LENGTH)(x_train)\n",
    "    # print(m)\n",
    "    y_train = labels1[:-nb_validation_samples]\n",
    "    #print(y_train.shape)\n",
    "    zeros_train = zeros[:-nb_validation_samples]\n",
    "    time_train = timeInfo1[:-nb_validation_samples]\n",
    "    post_train = postInfo1[:-nb_validation_samples]\n",
    "    x_val = data1[-nb_validation_samples:]\n",
    "    y_val = labels1[-nb_validation_samples:]\n",
    "    zeros_test = zeros[-nb_validation_samples:]\n",
    "    time_test = timeInfo1[-nb_validation_samples:]\n",
    "    post_test = postInfo1[-nb_validation_samples:]\n",
    "    y_single = single_label1[-nb_validation_samples:]\n",
    "\n",
    "    print('Number of positive and negative posts in training and test set')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    # building Hierachical Attention network\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                POST_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True)\n",
    "\n",
    "    all_input = Input(shape=(MAX_SENT_LENGTH + 2,))\n",
    "    sentence_input = crop(1, 0, MAX_SENT_LENGTH)(all_input)  ##slice\n",
    "    time_input = crop(1, MAX_SENT_LENGTH, MAX_SENT_LENGTH + 2)(all_input)  ##slice\n",
    "    embed_dim = 200  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 200  # Hidden layer size in feed forward network inside transformer\n",
    "    vocab_size = 20000\n",
    "    embedding_layer1 = TokenAndPositionEmbedding(MAX_SENT_LENGTH, vocab_size, embed_dim)\n",
    "    x = embedding_layer1(sentence_input)\n",
    "    transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    l_trans = transformer_block1(x)\n",
    "    l_att = Flatten()(l_trans)\n",
    "    l_att = Dense(200, activation='sigmoid')(l_att)\n",
    "    merged_output = Concatenate()([l_att, time_input])  ###text+time information\n",
    "    sentEncoder = Model(all_input, merged_output)\n",
    "\n",
    "    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH+2))\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    transformer_block2 = TransformerBlock(202, num_heads, ff_dim)\n",
    "    l_lstm_sent = transformer_block2(review_encoder)\n",
    "    # pred_time=Dense(1,activation='relu')(l_lstm_sent)\n",
    "    fully_sent = Dense(1, use_bias=False)(l_lstm_sent)\n",
    "    norm_fullysent = BatchNormalization()(fully_sent)\n",
    "    pred_time = Activation(activation='linear')(norm_fullysent)\n",
    "\n",
    "    zero_input = Input(shape=(1, 1))\n",
    "    shift_predtime = Concatenate(axis=1)([zero_input, pred_time])\n",
    "    shift_predtime = crop(1, 0, MAX_SENTS)(shift_predtime)\n",
    "    l_att_sent = Flatten()(l_lstm_sent)\n",
    "    l_att_sent = Dense(200, activation='sigmoid')(l_att_sent)\n",
    "\n",
    "    ###embed the #likes, shares\n",
    "    post_input = Input(shape=(1,))\n",
    "    #print(post_input)\n",
    "    # post_embedding = Dense(INFO_DIM, activation='sigmoid')(post_input)\n",
    "    fully_post = Dense(INFO_DIM, use_bias=False)(post_input)\n",
    "    norm_fullypost = BatchNormalization()(fully_post)\n",
    "    post_embedding = Activation(activation='relu')(norm_fullypost)\n",
    "    x = concatenate([l_att_sent,\n",
    "                     post_embedding])  ###merge the document level vectro with the additional embedded features such as #likes\n",
    "    fully_review = Dense(2, use_bias=False)(x)\n",
    "    norm_fullyreview = BatchNormalization()(fully_review)\n",
    "    preds = Activation(activation='softmax')(norm_fullyreview)\n",
    "\n",
    "    rmsprop = optimizers.Adam(learning_rate=0.001,  )\n",
    "    model = Model(inputs=[review_input, post_input, zero_input], outputs=[preds, shift_predtime])\n",
    "    # print(model.summary())\n",
    "    model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 0.00002],\n",
    "                  optimizer=rmsprop)\n",
    "    # filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "    # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\n",
    "\n",
    "    print(\"model fitting - Hierachical attention network for cyberbullying detection\")\n",
    "\n",
    "    model.fit([x_train, post_train, zeros_train], [y_train, time_train], batch_size=64,\n",
    "              epochs=25, verbose=1)\n",
    "    yp = model.predict([x_val, post_test, zeros_test], verbose=1)\n",
    "    ypreds = yp[0]\n",
    "    ypreds = np.argmax(ypreds, axis=1)\n",
    "    #print y_single\n",
    "    #print ypred\n",
    "    f1=precision_recall_fscore_support(y_single.astype(int), ypreds)   # <==\n",
    "    auc=roc_auc_score(y_single.astype('int'), ypreds)  #<==\n",
    "    f1 = precision_recall_fscore_support(y_single.astype(int), ypreds)  # <==\n",
    "    auc = roc_auc_score(y_single.astype('int'), ypreds)  # <== category\n",
    "    end_time = process_time()\n",
    "    cpu_time = end_time - start_time\n",
    "    print(cpu_time)\n",
    "    print(f1)\n",
    "    print(auc)\n",
    "    HAN_TIME.append(cpu_time)\n",
    "    HAN_AUC.append(auc)\n",
    "    HAN_f1.append(f1[2][1])\n",
    "    HAN_reca.append(f1[1][1])\n",
    "    HAN_pre.append(f1[0][1])\n",
    "\n",
    "    #for t-sne visualization\n",
    "    # if j==0:\n",
    "    #     a=model.layers\n",
    "    #     get_representations_test = K.function([model.layers[0].input,model.layers[1].input,model.layers[12].input], [model.layers[6].output])\n",
    "    #     representations_test = get_representations_test([x_val,post_test,zeros_test])[0]\n",
    "    #     representation_dict = {\n",
    "    #         'representations': representations_test,\n",
    "    #         'labels': y_single\n",
    "    #     }\n",
    "    #\n",
    "    #     with open('HANCD_Tem_results.pickle', 'wb') as handle:\n",
    "    #         pickle.dump(representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # K.clear_session()\n",
    "\n",
    "print(HAN_AUC)\n",
    "print(HAN_f1)\n",
    "print(HAN_pre)\n",
    "print(HAN_reca)\n",
    "print(HAN_TIME)\n",
    "print (\"TIME\",np.mean(HAN_TIME), np.std(HAN_TIME))\n",
    "print (\"AUC\",np.mean(HAN_AUC), np.std(HAN_AUC))\n",
    "print (\"f1\", np.mean(HAN_f1), np.std(HAN_f1))\n",
    "print (\"precision\",np.mean(HAN_pre), np.std(HAN_pre))\n",
    "print (\"recall\", np.mean(HAN_reca), np.std(HAN_reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9c806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361a449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471e13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
