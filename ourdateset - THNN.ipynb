{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4769e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\keras\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid</th>\n",
       "      <th>text</th>\n",
       "      <th>comment</th>\n",
       "      <th>time</th>\n",
       "      <th>poster</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MFl4csvfW</td>\n",
       "      <td>原来大家所恐惧的16号裁判正是令科尔口吐口香糖的那位金州勇士#湖人对战勇士#LLOVE_维...</td>\n",
       "      <td>['就是他赌球', '人家可是歪嘴战神', '嘴气歪的那位', '所以呢主场给安排凯尔特人不...</td>\n",
       "      <td>[05月06日 22:39, 05月06日 22:26, 05月06日 22:23, 05月...</td>\n",
       "      <td>167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MF8iLkA7H</td>\n",
       "      <td>#湖人vs勇士#@H和30:《偷一个主场就够了》《金州裁判队》《克莱不可能场场这样》《今天湖...</td>\n",
       "      <td>['勇蜜又如意了是吧', '去年太阳输独行侠第三场时也是铺天盖地的太阳放水论[送花花]', ...</td>\n",
       "      <td>[05月05日 16:22, 05月05日 17:15, 05月05日 15:58, 05月...</td>\n",
       "      <td>79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MF8Qjy6UC</td>\n",
       "      <td>我发了一条这个比赛勇士输不了，不知道为什么很多人理解成我说的是裁判。裁判尺度肯定是会影响比赛...</td>\n",
       "      <td>['gswin5', '是的，开场就懒懒散散的，加上裁判这么照顾浓眉，那就没得打了', '裁...</td>\n",
       "      <td>[05月05日 20:38, 05月05日 20:28, 05月05日 18:55, 05月...</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MF7eQ72mG</td>\n",
       "      <td>花了是勇士三分投的准，湖人自己防守差，打得不好，跟裁判也没关系。我也不认可假球的说法，我没觉...</td>\n",
       "      <td>['湖人球迷急了', '但是湖人没上强度也是真的，感觉就是在投篮训练，互相投篮就被勇士投花了...</td>\n",
       "      <td>[05月05日 11:06, 05月05日 11:06, 05月05日 11:07, 05月...</td>\n",
       "      <td>429</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MF7dZ7U1S</td>\n",
       "      <td>这要是格林和施罗德角色互换一下...球迷能批斗谩骂整个季后赛裁判:由于格林有什么什么历史，所...</td>\n",
       "      <td>['真的容易拉伤内侧肌肉', '施罗德小动作很多的', '詹密看不见系列', '裤蜜看得见？...</td>\n",
       "      <td>[05月05日 11:08, 05月05日 11:43, 05月05日 19:42, 05月...</td>\n",
       "      <td>378</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Mp3Pmr9CB</td>\n",
       "      <td>2022年，我们在让人窒息的疫情间隙里为了一场又一场幸福奔走。广东、福建、浙江、江苏、安徽、...</td>\n",
       "      <td>['', '怎么做到的，太牛了！', '祖国大好河山']</td>\n",
       "      <td>[01月20日 09:57, 01月20日 09:50, 01月19日 22:01]</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>MoV7e2LHs</td>\n",
       "      <td>胡明轩小胡签名签到忘我，上了辽宁队的车，小胡：赶紧溜，你们啥也没看见#胡明轩##广东宏远#...</td>\n",
       "      <td>['哈哈哈哈哈哈哈你的笔拿回来了吗', '🐯：快跑丢死个人', '对不起哥但是还是想笑哈哈哈...</td>\n",
       "      <td>[01月18日 22:52, 01月18日 22:56, 01月18日 22:59, 01月...</td>\n",
       "      <td>557</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>MoZqCfoA5</td>\n",
       "      <td>辽粤大战｜郭艾伦14分6篮板6助攻，张镇麟10分4篮板3助攻2断｜广东VS辽宁｜CBA常规赛...</td>\n",
       "      <td>['是的，杜峰太了解了他了，只要郭艾伦持球就知道他下一个动作了，所以真的跟困难，虽然说不能找...</td>\n",
       "      <td>[01月19日 13:56, 01月19日 11:43, 01月19日 11:05, 01月...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>MpvTn2kBR</td>\n",
       "      <td>今年只有央视春晚的观众没有戴口罩，其它地方台（包括北京卫视，上海卫视，辽宁卫视，东方卫视，广...</td>\n",
       "      <td>['地方提前录的，那时候不清楚央视态度，求稳就戴着了', '地方台提前更早录好的吧', '不...</td>\n",
       "      <td>[01月22日 21:27, 01月22日 20:28, 01月23日 08:40, 01月...</td>\n",
       "      <td>268</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>MpUPCF0Vr</td>\n",
       "      <td>#春节旅游#【兔年春节过半，太原门票预订同比涨952%】1月25日，兔年春节假期已经过半。携...</td>\n",
       "      <td>['大家都不怕疫情了', '春节大家都去旅游了', '大家都想出去走走了', '有空去', ...</td>\n",
       "      <td>[01月25日 12:33, 01月25日 12:08, 01月25日 12:00, 01月...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bid                                               text  \\\n",
       "0    MFl4csvfW  原来大家所恐惧的16号裁判正是令科尔口吐口香糖的那位金州勇士#湖人对战勇士#LLOVE_维...   \n",
       "1    MF8iLkA7H  #湖人vs勇士#@H和30:《偷一个主场就够了》《金州裁判队》《克莱不可能场场这样》《今天湖...   \n",
       "2    MF8Qjy6UC  我发了一条这个比赛勇士输不了，不知道为什么很多人理解成我说的是裁判。裁判尺度肯定是会影响比赛...   \n",
       "3    MF7eQ72mG  花了是勇士三分投的准，湖人自己防守差，打得不好，跟裁判也没关系。我也不认可假球的说法，我没觉...   \n",
       "4    MF7dZ7U1S  这要是格林和施罗德角色互换一下...球迷能批斗谩骂整个季后赛裁判:由于格林有什么什么历史，所...   \n",
       "..         ...                                                ...   \n",
       "820  Mp3Pmr9CB  2022年，我们在让人窒息的疫情间隙里为了一场又一场幸福奔走。广东、福建、浙江、江苏、安徽、...   \n",
       "821  MoV7e2LHs  胡明轩小胡签名签到忘我，上了辽宁队的车，小胡：赶紧溜，你们啥也没看见#胡明轩##广东宏远#...   \n",
       "822  MoZqCfoA5  辽粤大战｜郭艾伦14分6篮板6助攻，张镇麟10分4篮板3助攻2断｜广东VS辽宁｜CBA常规赛...   \n",
       "823  MpvTn2kBR  今年只有央视春晚的观众没有戴口罩，其它地方台（包括北京卫视，上海卫视，辽宁卫视，东方卫视，广...   \n",
       "824  MpUPCF0Vr  #春节旅游#【兔年春节过半，太原门票预订同比涨952%】1月25日，兔年春节假期已经过半。携...   \n",
       "\n",
       "                                               comment  \\\n",
       "0    ['就是他赌球', '人家可是歪嘴战神', '嘴气歪的那位', '所以呢主场给安排凯尔特人不...   \n",
       "1    ['勇蜜又如意了是吧', '去年太阳输独行侠第三场时也是铺天盖地的太阳放水论[送花花]', ...   \n",
       "2    ['gswin5', '是的，开场就懒懒散散的，加上裁判这么照顾浓眉，那就没得打了', '裁...   \n",
       "3    ['湖人球迷急了', '但是湖人没上强度也是真的，感觉就是在投篮训练，互相投篮就被勇士投花了...   \n",
       "4    ['真的容易拉伤内侧肌肉', '施罗德小动作很多的', '詹密看不见系列', '裤蜜看得见？...   \n",
       "..                                                 ...   \n",
       "820                       ['', '怎么做到的，太牛了！', '祖国大好河山']   \n",
       "821  ['哈哈哈哈哈哈哈你的笔拿回来了吗', '🐯：快跑丢死个人', '对不起哥但是还是想笑哈哈哈...   \n",
       "822  ['是的，杜峰太了解了他了，只要郭艾伦持球就知道他下一个动作了，所以真的跟困难，虽然说不能找...   \n",
       "823  ['地方提前录的，那时候不清楚央视态度，求稳就戴着了', '地方台提前更早录好的吧', '不...   \n",
       "824  ['大家都不怕疫情了', '春节大家都去旅游了', '大家都想出去走走了', '有空去', ...   \n",
       "\n",
       "                                                  time  poster  lable  \n",
       "0    [05月06日 22:39, 05月06日 22:26, 05月06日 22:23, 05月...     167    0.0  \n",
       "1    [05月05日 16:22, 05月05日 17:15, 05月05日 15:58, 05月...      79    0.0  \n",
       "2    [05月05日 20:38, 05月05日 20:28, 05月05日 18:55, 05月...      69    0.0  \n",
       "3    [05月05日 11:06, 05月05日 11:06, 05月05日 11:07, 05月...     429    1.0  \n",
       "4    [05月05日 11:08, 05月05日 11:43, 05月05日 19:42, 05月...     378    1.0  \n",
       "..                                                 ...     ...    ...  \n",
       "820         [01月20日 09:57, 01月20日 09:50, 01月19日 22:01]      53    0.0  \n",
       "821  [01月18日 22:52, 01月18日 22:56, 01月18日 22:59, 01月...     557    0.0  \n",
       "822  [01月19日 13:56, 01月19日 11:43, 01月19日 11:05, 01月...      55    0.0  \n",
       "823  [01月22日 21:27, 01月22日 20:28, 01月23日 08:40, 01月...     268    0.0  \n",
       "824  [01月25日 12:33, 01月25日 12:08, 01月25日 12:00, 01月...      38    0.0  \n",
       "\n",
       "[825 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Concatenate, BatchNormalization, Activation, Flatten,GlobalAveragePooling1D\n",
    "from keras.layers import Lambda, Embedding, GRU, Bidirectional, TimeDistributed, concatenate\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from word2vecReader import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "# from tensorflow.python.keras.optimizers import adam_v2\n",
    "# from tensorflow.python.keras.optimizers import rmsprop_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from  keras import layers\n",
    "from time import process_time\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "np.random.seed(0)\n",
    "MAX_SENT_LENGTH = 20  #number of words in a sentence\n",
    "MAX_NB_WORDS = 20000\n",
    "POST_DIM = 123\n",
    "INFO_DIM = 30\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "##slice tensor function in keras\n",
    "def crop(dimension, start, end):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "\n",
    "    return Lambda(func)\n",
    "\n",
    "\n",
    "def myFunc(x):\n",
    "    if \"empety\" in x:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = string.strip().lower()\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_words = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def find_str(s, char):\n",
    "    index = 0\n",
    "\n",
    "    if char in s:\n",
    "        c = char[0]\n",
    "        for ch in s:\n",
    "            if ch == c:\n",
    "                if s[index:index + len(char)] == char:\n",
    "                    return index\n",
    "\n",
    "            index += 1\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weightss = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    \n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''Transformer的Encoder部分'''\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "'''Transformer输入的编码层'''\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = 'C:/Users/Administrator/Desktop/keyan dateset/微博正文'\n",
    "\n",
    "# 创建包含三个关键字的字典\n",
    "data_dict = {\n",
    "    'bid': [],\n",
    "    'text': [],\n",
    "    'comment': [],\n",
    "    'time': [],\n",
    "    'poster': [],\n",
    "    'lable':[]\n",
    "}\n",
    "\n",
    "# 遍历文件夹中的所有文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # 打开 Excel 文件\n",
    "        workbook = openpyxl.load_workbook(file_path)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        # 提取数据\n",
    "        for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "            bid = row[1]\n",
    "            text = row[5]\n",
    "            likes = row[8]\n",
    "\n",
    "            # 将数据添加到字典对应的列表中\n",
    "            data_dict['bid'].append(bid)\n",
    "            data_dict['text'].append(text)\n",
    "            data_dict['poster'].append(likes)\n",
    "\n",
    "        # 关闭 Excel 文件\n",
    "        workbook.close()\n",
    "\n",
    "bid_length = len(data_dict['bid'])\n",
    "for i,bid in enumerate(data_dict['bid']):\n",
    "    # 构建文件路径\n",
    "    for root, directories, files in os.walk('C:/Users/Administrator/Desktop/keyan dateset/comment'):\n",
    "        for file in files:\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            if(str(bid)==str(file_name)):\n",
    "                file_path = os.path.join(root, file)\n",
    "                #print(file_path)\n",
    "                workbook = xlrd.open_workbook(file_path)\n",
    "                sheet = workbook.sheet_by_index(0)  # 假设要读取第一个工作表\n",
    "\n",
    "                # 遍历工作表的行\n",
    "                comment=[]\n",
    "                time=[]\n",
    "                lable=sheet.cell_value(1, 5)\n",
    "                cell = sheet.cell(1, 5)\n",
    "                lable = cell.value\n",
    "\n",
    "                # 如果单元格为空，将其视为0\n",
    "                if cell.ctype == xlrd.XL_CELL_EMPTY:\n",
    "                    lable = 0\n",
    "\n",
    "                for row in range(1, sheet.nrows):\n",
    "                    comment1 = sheet.cell_value(row,2)\n",
    "                    time1 = sheet.cell_value(row,3)\n",
    "                    comment.append(comment1)\n",
    "                    time.append(time1)\n",
    "                data_dict['lable'].append(lable)\n",
    "                data_dict['comment'].append(comment)\n",
    "                data_dict['time'].append(time)\n",
    "        # 关闭 Excel 文件\n",
    "                workbook.release_resources()\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# 删除comment列为空列表的行\n",
    "df = df[df['comment'].apply(len) > 0]\n",
    "df = df[df['time'].apply(lambda x: len(x[0]) != 6)]\n",
    "# 删除comment和time具有相同值的行\n",
    "df = df[~((df['comment'] == df['time']) & (df['comment'].apply(len) > 0))]\n",
    "df['comment'] = df['comment'].astype(str)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\<.*?\\>', '', x))  # 去除方括号及其内容\n",
    "df['comment'] = df['comment'].apply(lambda x: re.sub(r'\\<.*?\\>', '', x))  # 去除方括号及其内容\n",
    "# 重置索引\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 打印 DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b5135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time=df['time']\n",
    "\n",
    "time1_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe78300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_list in time:\n",
    "    if len(time_list[0]) > 6:\n",
    "        base_time = datetime.strptime(time_list[0], '%m月%d日 %H:%M')\n",
    "    else:\n",
    "        base_time = datetime.strptime(time_list[0] + ' 00:00', '%m月%d日 %H:%M')\n",
    "    time_diff_list = []\n",
    "    for time_str in time_list:\n",
    "        if len(time_str) > 8:\n",
    "            current_time = datetime.strptime(time_str, '%m月%d日 %H:%M')\n",
    "        else:\n",
    "            current_time = datetime.strptime(time_str + ' 00:00', '%m月%d日 %H:%M')\n",
    "        time_diff = (current_time - base_time).total_seconds()\n",
    "        time_diff_list.append(time_diff)\n",
    "    time1_list.append(time_diff_list)\n",
    "#print(time1_list)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992f014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 7189 unique tokens.\n",
      "Shape of data tensor: (825, 490, 21)\n",
      "Shape of label tensor: (825, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: overflow encountered in square\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: invalid value encountered in square\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "E:\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\Administrator\\Desktop\\keyan dateset\\word2vecReader.py:166: RuntimeWarning: invalid value encountered in divide\n",
      "  self.syn0norm = (self.syn0 / sqrt((self.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "texts = df['text']\n",
    "texts = texts.fillna(\"\")\n",
    "# texts=[text.encode('ascii') for text in texts]\n",
    "#print(texts)\n",
    "comments = df['comment']\n",
    "#print(comments)\n",
    "timeInfo = time1_list\n",
    "#print(timeInfo)\n",
    "postInfo = df['poster']\n",
    "#print(postInfo)\n",
    "\n",
    "labels = df['lable']\n",
    "#print(labels)\n",
    "b = np.zeros([len(timeInfo), len(max(timeInfo, key=lambda x: len(x)))])\n",
    "for i, j in enumerate(timeInfo):\n",
    "    b[i][0:len(j)] = j\n",
    "timeInfo = b\n",
    "time_size = len(np.unique(timeInfo))\n",
    "MAX_SENTS = len(timeInfo[0])  ####number of sentences\n",
    "\n",
    "postInfo = np.array(postInfo)\n",
    "#print(postInfo)\n",
    "post_size = len(np.unique(postInfo))\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "#tokenizer.fit_on_texts(texts)\n",
    "#print(MAX_SENTS)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH+1), dtype='int32')\n",
    "#print(tokenizer.word_index['club'])\n",
    "\n",
    "texts = texts.apply(lambda x: list(jieba.cut(x)))\n",
    "word_count = Counter()\n",
    "for text_seg in texts:\n",
    "    word_count.update(text_seg)\n",
    "word_list = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = {}\n",
    "for i, (word, freq) in enumerate(word_list):\n",
    "    vocab[word] = i\n",
    "\n",
    "#comments = comments.apply(lambda x: list(jieba.cut(x)))\n",
    "#comments = comments.apply(lambda x: [vocab[word] for word in x if word in vocab])\n",
    "m = 0\n",
    "n = 0\n",
    "for m, sentences in enumerate(comments):\n",
    "    sentences = sentences.split(',')\n",
    "    for n, sent in enumerate(sentences):\n",
    "        if n < MAX_SENTS:\n",
    "            k = 0\n",
    "            sent = list(jieba.cut(sent))\n",
    "            for word in sent:\n",
    "                if word in vocab:\n",
    "                    data[m, n, k] = vocab[word]\n",
    "                    k = k + 1\n",
    "                if k >= MAX_SENT_LENGTH:\n",
    "                    break\n",
    "\n",
    "word_index = vocab\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "single_label = np.asarray(labels)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "###standardize\n",
    "timeInfo = preprocessing.StandardScaler().fit_transform(timeInfo)\n",
    "postInfo = preprocessing.StandardScaler().fit_transform(postInfo.reshape(-1, 1))\n",
    "HAN_pre = []\n",
    "HAN_reca = []\n",
    "HAN_f1 = []\n",
    "HAN_AUC = []\n",
    "HAN_TIME = []\n",
    "embeddings_index = Word2Vec.load_word2vec_format(\"word2vec_model.bin\", binary=True, )  #\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, POST_DIM))\n",
    "outword_dic = dict()\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        new_vector = np.random.rand(POST_DIM, )\n",
    "        outword_dic.setdefault(word, new_vector)\n",
    "        embedding_matrix[i] = outword_dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f690e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825, 490, 1)\n",
      "Number of positive and negative posts in training and test set\n",
      "[391. 269.]\n",
      "[107.  58.]\n",
      "model fitting - Hierachical attention network for cyberbullying detection\n",
      "Epoch 1/25\n",
      "10/11 [==========================>...] - ETA: 10s - loss: 0.6800 - activation_2_loss: 0.6800 - lambda_3_loss: 1.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 104\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel fitting - Hierachical attention network for cyberbullying detection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeros_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m yp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([x_val, post_test, zeros_test], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    107\u001b[0m ypreds \u001b[38;5;241m=\u001b[39m yp[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    start_time = process_time()\n",
    "    indices = np.arange(data.shape[0])\n",
    "    #print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    data1 = data[indices]\n",
    "    #print(data1.shape)\n",
    "    labels1 = labels[indices]\n",
    "    single_label1 = single_label[indices]\n",
    "    timeInfo1 = timeInfo[indices]\n",
    "    #print(timeInfo1.shape)\n",
    "    timeInfo1 = timeInfo1.reshape((825, MAX_SENTS, 1))\n",
    "    print(timeInfo1.shape)\n",
    "    data1 = np.dstack((data1, timeInfo1))\n",
    "    postInfo1=postInfo[indices]\n",
    "    #print(postInfo1.shape)\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data1.shape[0])\n",
    "    zeros = np.zeros(825)\n",
    "    zeros = zeros.reshape((825, 1, 1))\n",
    "\n",
    "    x_train = data1[:-nb_validation_samples]\n",
    "    m=crop(1, 0, MAX_SENT_LENGTH)(x_train)\n",
    "    # print(m)\n",
    "    y_train = labels1[:-nb_validation_samples]\n",
    "    #print(y_train.shape)\n",
    "    zeros_train = zeros[:-nb_validation_samples]\n",
    "    time_train = timeInfo1[:-nb_validation_samples]\n",
    "    post_train = postInfo1[:-nb_validation_samples]\n",
    "    x_val = data1[-nb_validation_samples:]\n",
    "    y_val = labels1[-nb_validation_samples:]\n",
    "    zeros_test = zeros[-nb_validation_samples:]\n",
    "    time_test = timeInfo1[-nb_validation_samples:]\n",
    "    post_test = postInfo1[-nb_validation_samples:]\n",
    "    y_single = single_label1[-nb_validation_samples:]\n",
    "\n",
    "    print('Number of positive and negative posts in training and test set')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    # building Hierachical Attention network\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                POST_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True)\n",
    "\n",
    "    all_input = Input(shape=(MAX_SENT_LENGTH + 2,))\n",
    "    sentence_input = crop(1, 0, MAX_SENT_LENGTH)(all_input)  ##slice\n",
    "    time_input = crop(1, MAX_SENT_LENGTH, MAX_SENT_LENGTH + 2)(all_input)  ##slice\n",
    "    embed_dim = 200  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 200  # Hidden layer size in feed forward network inside transformer\n",
    "    vocab_size = 20000\n",
    "    embedding_layer1 = TokenAndPositionEmbedding(MAX_SENT_LENGTH, vocab_size, embed_dim)\n",
    "    x = embedding_layer1(sentence_input)\n",
    "    transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    l_trans = transformer_block1(x)\n",
    "    l_att = Flatten()(l_trans)\n",
    "    l_att = Dense(200, activation='sigmoid')(l_att)\n",
    "    merged_output = Concatenate()([l_att, time_input])  ###text+time information\n",
    "    sentEncoder = Model(all_input, merged_output)\n",
    "\n",
    "    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH+2))\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    transformer_block2 = TransformerBlock(202, num_heads, ff_dim)\n",
    "    l_lstm_sent = transformer_block2(review_encoder)\n",
    "    # pred_time=Dense(1,activation='relu')(l_lstm_sent)\n",
    "    fully_sent = Dense(1, use_bias=False)(l_lstm_sent)\n",
    "    norm_fullysent = BatchNormalization()(fully_sent)\n",
    "    pred_time = Activation(activation='linear')(norm_fullysent)\n",
    "\n",
    "    zero_input = Input(shape=(1, 1))\n",
    "    shift_predtime = Concatenate(axis=1)([zero_input, pred_time])\n",
    "    shift_predtime = crop(1, 0, MAX_SENTS)(shift_predtime)\n",
    "    l_att_sent = Flatten()(l_lstm_sent)\n",
    "    l_att_sent = Dense(200, activation='sigmoid')(l_att_sent)\n",
    "\n",
    "    ###embed the #likes, shares\n",
    "    post_input = Input(shape=(1,))\n",
    "    #print(post_input)\n",
    "    # post_embedding = Dense(INFO_DIM, activation='sigmoid')(post_input)\n",
    "    fully_post = Dense(INFO_DIM, use_bias=False)(post_input)\n",
    "    norm_fullypost = BatchNormalization()(fully_post)\n",
    "    post_embedding = Activation(activation='relu')(norm_fullypost)\n",
    "    x = concatenate([l_att_sent,\n",
    "                     post_embedding])  ###merge the document level vectro with the additional embedded features such as #likes\n",
    "    fully_review = Dense(2, use_bias=False)(x)\n",
    "    norm_fullyreview = BatchNormalization()(fully_review)\n",
    "    preds = Activation(activation='softmax')(norm_fullyreview)\n",
    "\n",
    "    rmsprop = optimizers.Adam(learning_rate=0.001,  )\n",
    "    model = Model(inputs=[review_input, post_input, zero_input], outputs=[preds, shift_predtime])\n",
    "    # print(model.summary())\n",
    "    model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 0.00002],\n",
    "                  optimizer=rmsprop)\n",
    "    # filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "    # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    # callbacks_list = [EarlyStopping(monitor='loss', patience=1,mode='min'),checkpoint]\n",
    "\n",
    "    print(\"model fitting - Hierachical attention network for cyberbullying detection\")\n",
    "\n",
    "    model.fit([x_train, post_train, zeros_train], [y_train, time_train], batch_size=64,\n",
    "              epochs=25, verbose=1)\n",
    "    yp = model.predict([x_val, post_test, zeros_test], verbose=1)\n",
    "    ypreds = yp[0]\n",
    "    ypreds = np.argmax(ypreds, axis=1)\n",
    "    #print y_single\n",
    "    #print ypred\n",
    "    f1=precision_recall_fscore_support(y_single.astype(int), ypreds)   # <==\n",
    "    auc=roc_auc_score(y_single.astype('int'), ypreds)  #<==\n",
    "    f1 = precision_recall_fscore_support(y_single.astype(int), ypreds)  # <==\n",
    "    auc = roc_auc_score(y_single.astype('int'), ypreds)  # <== category\n",
    "    end_time = process_time()\n",
    "    cpu_time = end_time - start_time\n",
    "    print(cpu_time)\n",
    "    print(f1)\n",
    "    print(auc)\n",
    "    HAN_TIME.append(cpu_time)\n",
    "    HAN_AUC.append(auc)\n",
    "    HAN_f1.append(f1[2][1])\n",
    "    HAN_reca.append(f1[1][1])\n",
    "    HAN_pre.append(f1[0][1])\n",
    "\n",
    "    #for t-sne visualization\n",
    "    # if j==0:\n",
    "    #     a=model.layers\n",
    "    #     get_representations_test = K.function([model.layers[0].input,model.layers[1].input,model.layers[12].input], [model.layers[6].output])\n",
    "    #     representations_test = get_representations_test([x_val,post_test,zeros_test])[0]\n",
    "    #     representation_dict = {\n",
    "    #         'representations': representations_test,\n",
    "    #         'labels': y_single\n",
    "    #     }\n",
    "    #\n",
    "    #     with open('HANCD_Tem_results.pickle', 'wb') as handle:\n",
    "    #         pickle.dump(representation_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # K.clear_session()\n",
    "\n",
    "print(HAN_AUC)\n",
    "print(HAN_f1)\n",
    "print(HAN_pre)\n",
    "print(HAN_reca)\n",
    "print(HAN_TIME)\n",
    "print (\"TIME\",np.mean(HAN_TIME), np.std(HAN_TIME))\n",
    "print (\"AUC\",np.mean(HAN_AUC), np.std(HAN_AUC))\n",
    "print (\"f1\", np.mean(HAN_f1), np.std(HAN_f1))\n",
    "print (\"precision\",np.mean(HAN_pre), np.std(HAN_pre))\n",
    "print (\"recall\", np.mean(HAN_reca), np.std(HAN_reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9c806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361a449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471e13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
